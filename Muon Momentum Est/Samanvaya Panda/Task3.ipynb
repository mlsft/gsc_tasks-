{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Kls0YzDcfsHS",
    "outputId": "9a02bcb6-822e-46af-bc45-b04e56db2d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (20.0.2)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.47.0)\n",
      "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.21.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.25.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
      "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.0)\n",
      "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.2)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (7.0.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.0.0)\n",
      "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.6)\n",
      "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.1.0)\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
      "Collecting torch-sparse==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (15.2 MB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.18.2)\n",
      "Installing collected packages: torch-sparse\n",
      "  Attempting uninstall: torch-sparse\n",
      "    Found existing installation: torch-sparse 0.6.1\n",
      "    Uninstalling torch-sparse-0.6.1:\n",
      "      Successfully uninstalled torch-sparse-0.6.1\n",
      "Successfully installed torch-sparse-0.6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch_sparse"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
      "Collecting torch-cluster==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (14.5 MB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster==latest+cu101) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster==latest+cu101) (1.18.2)\n",
      "Installing collected packages: torch-cluster\n",
      "  Attempting uninstall: torch-cluster\n",
      "    Found existing installation: torch-cluster 1.5.3\n",
      "    Uninstalling torch-cluster-1.5.3:\n",
      "      Successfully uninstalled torch-cluster-1.5.3\n",
      "Successfully installed torch-cluster-1.5.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch_cluster"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
      "Collecting torch-scatter==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (10.6 MB)\n",
      "Installing collected packages: torch-scatter\n",
      "  Attempting uninstall: torch-scatter\n",
      "    Found existing installation: torch-scatter 2.0.4\n",
      "    Uninstalling torch-scatter-2.0.4:\n",
      "      Successfully uninstalled torch-scatter-2.0.4\n",
      "Successfully installed torch-scatter-2.0.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch_scatter"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
      "Collecting torch-spline-conv==latest+cu101\n",
      "  Using cached https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.4.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (5.1 MB)\n",
      "Installing collected packages: torch-spline-conv\n",
      "  Attempting uninstall: torch-spline-conv\n",
      "    Found existing installation: torch-spline-conv 1.2.0\n",
      "    Uninstalling torch-spline-conv-1.2.0:\n",
      "      Successfully uninstalled torch-spline-conv-1.2.0\n",
      "Successfully installed torch-spline-conv-1.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch_spline_conv"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: energyflow in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from energyflow) (1.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.6/dist-packages (from energyflow) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from energyflow) (1.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch-geometric\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.4.0.html\n",
    "!pip install energyflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLOdUKcdW9_8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from torch_geometric.nn import MessagePassing, knn_graph\n",
    "import torch_geometric.transforms as T\n",
    "import energyflow\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJRevLPof24t"
   },
   "outputs": [],
   "source": [
    "x, y = energyflow.qg_jets.load(num_data=8*10**5, generator='pythia', pad=True, with_bc=False, cache_dir='~/.energyflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6JhB8wYZf-Cl"
   },
   "outputs": [],
   "source": [
    "data = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "label = np.array(y, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4WcEkXRf-yk"
   },
   "outputs": [],
   "source": [
    "class EdgeConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EdgeConv, self).__init__(aggr='max') #  \"Max\" aggregation.\n",
    "        self.mlp = Seq(Linear(2 * in_channels, out_channels),\n",
    "                       ReLU(),\n",
    "                       Linear(out_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_i, x_j):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "\n",
    "        tmp = torch.cat([x_i, x_j - x_i], dim=1)  # tmp has shape [E, 2 * in_channels]\n",
    "        return self.mlp(tmp)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "\n",
    "        return aggr_out\n",
    "    \n",
    "class DynamicEdgeConv(EdgeConv):\n",
    "    def __init__(self, in_channels, out_channels, k=30):\n",
    "        super(DynamicEdgeConv, self).__init__(in_channels, out_channels)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        edge_index = knn_graph(x, self.k, batch, loop=False, flow=self.flow)\n",
    "        return super(DynamicEdgeConv, self).forward(x, edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTOC4RbpgFF7"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = DynamicEdgeConv(data.shape[1], 128)\n",
    "        self.conv2 = DynamicEdgeConv(128, 256)\n",
    "        self.conv3 = DynamicEdgeConv(256, 128)\n",
    "        self.conv4 = DynamicEdgeConv(128, 32)\n",
    "        self.conv5 = DynamicEdgeConv(32, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YH0SfQgFXvM8"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "raw_data = []\n",
    "batch_size=100\n",
    "t_size = 2*10**5\n",
    "n = 10**5\n",
    "t_data = normalize(data[:t_size], axis=0)\n",
    "for i in range(int(t_data.shape[0]/batch_size)):\n",
    "    x = torch.from_numpy(t_data[i*batch_size:(i+1)*batch_size]).float()\n",
    "    y = torch.from_numpy(label[i*batch_size:(i+1)*batch_size])\n",
    "    raw_data.append(Data(x=x, y=y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YXQTMdwmgTJ6",
    "outputId": "ac1c01aa-60b6-497b-b45a-2ac70d1c462c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 0.6089878984093666 1000 0.5737396126091481 1500 0.5621184630990028 2000 0.5547156317234039 \n",
      "Epoch [1/75], Loss: 0.5547\n",
      "500 0.5325500974655152 1000 0.5331735238730907 1500 0.5334095007777214 2000 0.5323453467339277 \n",
      "Epoch [2/75], Loss: 0.5323\n",
      "500 0.5308255921006203 1000 0.5311992216408252 1500 0.5314167400995891 2000 0.5303901437222958 \n",
      "Epoch [3/75], Loss: 0.5304\n",
      "500 0.5281738523840904 1000 0.5287449304163456 1500 0.5290461152990659 2000 0.5281871314197779 \n",
      "Epoch [4/75], Loss: 0.5282\n",
      "500 0.5266513560414314 1000 0.5275274778306485 1500 0.527858126560847 2000 0.526847633779049 \n",
      "Epoch [5/75], Loss: 0.5268\n",
      "500 0.5255070666074753 1000 0.5262863366007805 1500 0.5266648045579593 2000 0.5257789367139339 \n",
      "Epoch [6/75], Loss: 0.5258\n",
      "500 0.5244067690372467 1000 0.5252837683260441 1500 0.5258563279906908 2000 0.5249658707827329 \n",
      "Epoch [7/75], Loss: 0.5250\n",
      "500 0.5238183180689812 1000 0.5248047896027564 1500 0.5249836120406787 2000 0.524130471482873 \n",
      "Epoch [8/75], Loss: 0.5241\n",
      "500 0.5223714443445205 1000 0.5235403705537319 1500 0.5242616382837295 2000 0.523467279523611 \n",
      "Epoch [9/75], Loss: 0.5235\n",
      "500 0.5216273231506348 1000 0.5224744397699833 1500 0.5229634839892388 2000 0.5222171033918858 \n",
      "Epoch [10/75], Loss: 0.5222\n",
      "500 0.5217890864610673 1000 0.5231200762093067 1500 0.5231893126567205 2000 0.5221359485983849 \n",
      "Epoch [11/75], Loss: 0.5221\n",
      "500 0.5205975842475891 1000 0.5216486784219742 1500 0.5219999204675356 2000 0.5214321182966233 \n",
      "Epoch [12/75], Loss: 0.5214\n",
      "500 0.520443095088005 1000 0.5214183579981327 1500 0.5216340605020523 2000 0.5207787852436304 \n",
      "Epoch [13/75], Loss: 0.5208\n",
      "500 0.5196241987943649 1000 0.5207223164737225 1500 0.521257263580958 2000 0.5203989394754172 \n",
      "Epoch [14/75], Loss: 0.5204\n",
      "500 0.5187128944993019 1000 0.5201642904281616 1500 0.5205048699577649 2000 0.5196186704188586 \n",
      "Epoch [15/75], Loss: 0.5196\n",
      "500 0.5174817840456962 1000 0.5193146779239177 1500 0.5197550455331802 2000 0.5190243087261915 \n",
      "Epoch [16/75], Loss: 0.5190\n",
      "500 0.5175619970560074 1000 0.5190270825028419 1500 0.5195861562291781 2000 0.5187145981788636 \n",
      "Epoch [17/75], Loss: 0.5187\n",
      "500 0.5161245031356811 1000 0.5176358886659146 1500 0.5182732335925102 2000 0.5176078848540783 \n",
      "Epoch [18/75], Loss: 0.5176\n",
      "500 0.5161165410280227 1000 0.517643974751234 1500 0.5178378078540167 2000 0.5170640817135572 \n",
      "Epoch [19/75], Loss: 0.5171\n",
      "500 0.5150993482470513 1000 0.5165985317230225 1500 0.5169619605938593 2000 0.5162107866555452 \n",
      "Epoch [20/75], Loss: 0.5162\n",
      "500 0.5156423543095588 1000 0.5168108621537686 1500 0.5168712858359019 2000 0.5161377335637808 \n",
      "Epoch [21/75], Loss: 0.5161\n",
      "500 0.5148309519886971 1000 0.5155746894478798 1500 0.5157588838338852 2000 0.5150742492824792 \n",
      "Epoch [22/75], Loss: 0.5151\n",
      "500 0.5135629168748855 1000 0.515006422072649 1500 0.5154663088520368 2000 0.5146800604462624 \n",
      "Epoch [23/75], Loss: 0.5147\n",
      "500 0.5133762212991715 1000 0.5148774905800819 1500 0.515076285580794 2000 0.5142398516535759 \n",
      "Epoch [24/75], Loss: 0.5142\n",
      "500 0.5130945864915848 1000 0.5138472159206867 1500 0.5141053568323454 2000 0.5134394927322865 \n",
      "Epoch [25/75], Loss: 0.5134\n",
      "500 0.5126421793103219 1000 0.5136475123763085 1500 0.514049954990546 2000 0.5131411115080118 \n",
      "Epoch [26/75], Loss: 0.5131\n",
      "500 0.5123104156851769 1000 0.5135194354355336 1500 0.5139289097189903 2000 0.5129757589548827 \n",
      "Epoch [27/75], Loss: 0.5130\n",
      "500 0.5122675309181214 1000 0.513221495449543 1500 0.5131776974201202 2000 0.5126469770371914 \n",
      "Epoch [28/75], Loss: 0.5126\n",
      "500 0.5109533826112748 1000 0.5130629841685295 1500 0.5134575607776641 2000 0.5126247896105051 \n",
      "Epoch [29/75], Loss: 0.5126\n",
      "500 0.5111335942745209 1000 0.5117994128763675 1500 0.5120261882742246 2000 0.5112984828352928 \n",
      "Epoch [30/75], Loss: 0.5113\n",
      "500 0.5105416730046273 1000 0.5118530309200287 1500 0.5121091500719388 2000 0.5113642457425595 \n",
      "Epoch [31/75], Loss: 0.5114\n",
      "500 0.5100061593651771 1000 0.5112202716171741 1500 0.5120291180411974 2000 0.5112590888738632 \n",
      "Epoch [32/75], Loss: 0.5113\n",
      "500 0.5109062268137932 1000 0.5116710533201695 1500 0.5115638104875883 2000 0.5107427182346582 \n",
      "Epoch [33/75], Loss: 0.5107\n",
      "500 0.5094460144042968 1000 0.5104588268101216 1500 0.5109479494094848 2000 0.5103477736115456 \n",
      "Epoch [34/75], Loss: 0.5103\n",
      "500 0.5094613853096962 1000 0.5103516766428947 1500 0.5103477250536282 2000 0.5097177130579948 \n",
      "Epoch [35/75], Loss: 0.5097\n",
      "500 0.5092135837674141 1000 0.5101372322142124 1500 0.5102379205226898 2000 0.50947403678298 \n",
      "Epoch [36/75], Loss: 0.5095\n",
      "500 0.5088004266023636 1000 0.5094657788276672 1500 0.5092970199386279 2000 0.508772664949298 \n",
      "Epoch [37/75], Loss: 0.5088\n",
      "500 0.5087918847203254 1000 0.5096898615658283 1500 0.5096436598300934 2000 0.5088933897465467 \n",
      "Epoch [38/75], Loss: 0.5089\n",
      "500 0.509471831202507 1000 0.509241290897131 1500 0.5093636044462522 2000 0.5085408905446529 \n",
      "Epoch [39/75], Loss: 0.5085\n",
      "500 0.5073751846551895 1000 0.5079705477952957 1500 0.5080276824633281 2000 0.5073628543317318 \n",
      "Epoch [40/75], Loss: 0.5074\n",
      "500 0.5079637066125869 1000 0.5083959086537361 1500 0.5084981062809626 2000 0.5080922787487507 \n",
      "Epoch [41/75], Loss: 0.5081\n",
      "500 0.5087648367881775 1000 0.5089243233203888 1500 0.508312536418438 2000 0.5074717176258564 \n",
      "Epoch [42/75], Loss: 0.5075\n",
      "500 0.5065816165804863 1000 0.5074633824825286 1500 0.507985054552555 2000 0.5072247345298528 \n",
      "Epoch [43/75], Loss: 0.5072\n",
      "500 0.5072796739339829 1000 0.5073740493953228 1500 0.5071498643557231 2000 0.506843647941947 \n",
      "Epoch [44/75], Loss: 0.5068\n",
      "500 0.5072945145964622 1000 0.5073743271529675 1500 0.5074100254178047 2000 0.5067866773158312 \n",
      "Epoch [45/75], Loss: 0.5068\n",
      "500 0.5059504294395447 1000 0.5061490165889263 1500 0.5063729451100032 2000 0.5055768402814865 \n",
      "Epoch [46/75], Loss: 0.5056\n",
      "500 0.5057013157010078 1000 0.506052893191576 1500 0.5059395153919856 2000 0.5053105839341879 \n",
      "Epoch [47/75], Loss: 0.5053\n",
      "500 0.5062348675727845 1000 0.5067087464034558 1500 0.5065834184487661 2000 0.5056271549761295 \n",
      "Epoch [48/75], Loss: 0.5056\n",
      "500 0.5048292717933655 1000 0.5059395848214626 1500 0.5059534257650375 2000 0.5052705219388008 \n",
      "Epoch [49/75], Loss: 0.5053\n",
      "500 0.5050901276469231 1000 0.5056803064346314 1500 0.5057868422269821 2000 0.5047913463562727 \n",
      "Epoch [50/75], Loss: 0.5048\n",
      "500 0.5035408186912537 1000 0.5045858761072158 1500 0.5043933113018672 2000 0.5037894830107689 \n",
      "Epoch [51/75], Loss: 0.5038\n",
      "500 0.5036407120227814 1000 0.5041976449489594 1500 0.5041575071016947 2000 0.503399691015482 \n",
      "Epoch [52/75], Loss: 0.5034\n",
      "500 0.5032498235106468 1000 0.5040701856613159 1500 0.5039282553593317 2000 0.5034936723262071 \n",
      "Epoch [53/75], Loss: 0.5035\n",
      "500 0.5069752544760704 1000 0.5058092021942139 1500 0.5047791326244672 2000 0.5037530718892813 \n",
      "Epoch [54/75], Loss: 0.5038\n",
      "500 0.5019943895339966 1000 0.5028463694155216 1500 0.502820129096508 2000 0.5021070823818445 \n",
      "Epoch [55/75], Loss: 0.5021\n",
      "500 0.5029129717350006 1000 0.5039350031018257 1500 0.5030195304751396 2000 0.5020192228704691 \n",
      "Epoch [56/75], Loss: 0.5020\n",
      "500 0.5011297286748886 1000 0.501973468720913 1500 0.5021622361540794 2000 0.5015167684406042 \n",
      "Epoch [57/75], Loss: 0.5015\n",
      "500 0.5011634832024574 1000 0.5025438662469387 1500 0.5024695836305618 2000 0.501578512787819 \n",
      "Epoch [58/75], Loss: 0.5016\n",
      "500 0.5018896003365516 1000 0.5030487726628781 1500 0.5027427870432536 2000 0.5018862278759479 \n",
      "Epoch [59/75], Loss: 0.5019\n",
      "500 0.5013917587995529 1000 0.5021988600194455 1500 0.5023147379358609 2000 0.5017409771680832 \n",
      "Epoch [60/75], Loss: 0.5017\n",
      "500 0.5014442167282105 1000 0.5022940373122692 1500 0.5019080071250598 2000 0.5014124556332826 \n",
      "Epoch [61/75], Loss: 0.5014\n",
      "500 0.501447123169899 1000 0.5021390091478825 1500 0.5018049118518829 2000 0.5011319584995508 \n",
      "Epoch [62/75], Loss: 0.5011\n",
      "500 0.5006592841148376 1000 0.501357746988535 1500 0.5008011165857315 2000 0.49995012883841994 \n",
      "Epoch [63/75], Loss: 0.5000\n",
      "500 0.49902795004844663 1000 0.5001417326033115 1500 0.4997410250902176 2000 0.49914732098579406 \n",
      "Epoch [64/75], Loss: 0.4991\n",
      "500 0.49781268507242205 1000 0.498576953202486 1500 0.4985353920261065 2000 0.49824554508924485 \n",
      "Epoch [65/75], Loss: 0.4982\n",
      "500 0.4976475566625595 1000 0.498699874073267 1500 0.4989023564855258 2000 0.4982572108656168 \n",
      "Epoch [66/75], Loss: 0.4983\n",
      "500 0.4974293495416641 1000 0.4988816880881786 1500 0.498574571232001 2000 0.497700796186924 \n",
      "Epoch [67/75], Loss: 0.4977\n",
      "500 0.49769312316179276 1000 0.4979393308460712 1500 0.4979834424058596 2000 0.49738728946447375 \n",
      "Epoch [68/75], Loss: 0.4974\n",
      "500 0.4973485943675041 1000 0.4979252528846264 1500 0.49778139624993006 2000 0.49729903692007066 \n",
      "Epoch [69/75], Loss: 0.4973\n",
      "500 0.4962368052005768 1000 0.4971073054671288 1500 0.4973835485180219 2000 0.49731095632910727 \n",
      "Epoch [70/75], Loss: 0.4973\n",
      "500 0.4958507617115974 1000 0.4970742390751839 1500 0.4967206455866496 2000 0.49608668188750743 \n",
      "Epoch [71/75], Loss: 0.4961\n",
      "500 0.4959524672627449 1000 0.4965395655930042 1500 0.4961550786097844 2000 0.4957483575195074 \n",
      "Epoch [72/75], Loss: 0.4957\n",
      "500 0.49554579418897626 1000 0.4959001648426056 1500 0.4966380709807078 2000 0.4962511071562767 \n",
      "Epoch [73/75], Loss: 0.4963\n",
      "500 0.4971986893415451 1000 0.4973595509529114 1500 0.4968122372428576 2000 0.49620389959216116 \n",
      "Epoch [74/75], Loss: 0.4962\n",
      "500 0.49476552349328995 1000 0.4959137351810932 1500 0.49548791295289996 2000 0.4955186716616154 \n",
      "Epoch [75/75], Loss: 0.4955\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# model.train()\n",
    "epochLoss = []\n",
    "num_epochs = 75\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0; cntr = 0\n",
    "    for batch in raw_data:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = loss_func(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        cntr += 1\n",
    "        if cntr%500 == 0:\n",
    "            print(cntr, total_loss/cntr, end=\" \")\n",
    "    print()\n",
    "    print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, total_loss/cntr))\n",
    "    epochLoss.append(total_loss/cntr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYYlLDZ8uJFs"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaBRaUaIgxl0"
   },
   "outputs": [],
   "source": [
    "v_data = normalize(data[3*t_size:3*t_size+n], axis=0)\n",
    "\n",
    "def gen_data(temp_data, k):\n",
    "  raw_data = []\n",
    "  for i in range(int(temp_data.shape[0]/batch_size)):\n",
    "      x = torch.from_numpy(temp_data[i*batch_size:(i+1)*batch_size]).float()\n",
    "      y = torch.from_numpy(label[k+i*batch_size:k+(i+1)*batch_size])\n",
    "      raw_data.append(Data(x=x, y=y))\n",
    "  return raw_data\n",
    "\n",
    "val_data = gen_data(v_data, 3*t_size)\n",
    "pred = np.zeros(len(val_data*batch_size), dtype='int')\n",
    "i=0\n",
    "for batch in val_data:\n",
    "  out = model(batch.to(device))\n",
    "  out = out.detach().cpu().numpy()\n",
    "  pred[i*batch_size:(i+1)*batch_size] = np.argmax(out, axis=1) \n",
    "  i += 1\n",
    "np.savez_compressed('./predicted1', pred1=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lIfm0cAIpGa"
   },
   "outputs": [],
   "source": [
    "pred = [[], [], []]\n",
    "with np.load('predicted1.npz') as file:\n",
    "    pred[0].append(file['pred1'])\n",
    "with np.load('predicted2.npz') as file:\n",
    "    pred[1].append(file['pred2'])\n",
    "with np.load('predicted3.npz') as file:\n",
    "    pred[2].append(file['pred3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing weight for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pvITUlMcI7-P",
    "outputId": "892b5471-8dbe-4f55-8525-8bd2e0edad52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76512 0.75535 0.76752]\n"
     ]
    }
   ],
   "source": [
    "count = np.zeros(3)\n",
    "for i in range(n):\n",
    "  for j in range(3):\n",
    "    if pred[j][0][i] == label[3*t_size+i]:\n",
    "      count[j] += 1\n",
    "count = count/n\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "Mv4v0BDML217",
    "outputId": "021c4d4e-43b1-4d6d-d748-5dd1118ee220",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): DynamicEdgeConv(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=1120, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): DynamicEdgeConv(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): DynamicEdgeConv(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv4): DynamicEdgeConv(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): DynamicEdgeConv(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model1 = Net().to(device)\n",
    "model1.load_state_dict(torch.load('./model1.pt'))\n",
    "model1.eval()\n",
    "\n",
    "model2 = Net().to(device)\n",
    "model2.load_state_dict(torch.load('./model2.pt'))\n",
    "model2.eval()\n",
    "\n",
    "model3 = Net().to(device)\n",
    "model3.load_state_dict(torch.load('./model3.pt'))\n",
    "model3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fb_nVaPvN7gY"
   },
   "outputs": [],
   "source": [
    "te_data = normalize(data[3*t_size+n:], axis=0)\n",
    "\n",
    "def gen_data(temp_data, k):\n",
    "  raw_data = []\n",
    "  for i in range(int(temp_data.shape[0]/batch_size)):\n",
    "      x = torch.from_numpy(temp_data[i*batch_size:(i+1)*batch_size]).float()\n",
    "      y = torch.from_numpy(label[k+i*batch_size:k+(i+1)*batch_size])\n",
    "      raw_data.append(Data(x=x, y=y))\n",
    "  return raw_data\n",
    "\n",
    "test_data = gen_data(te_data, 3*t_size)\n",
    "pred = np.zeros(len(test_data*batch_size), dtype='int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cWbgVAaOOywi",
    "outputId": "54b71134-2ccc-4c1f-d439-71b3f820c4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model1 = 76.519\n",
      "Accuracy of model2 = 75.374\n",
      "Accuracy of model3 = 76.538\n"
     ]
    }
   ],
   "source": [
    "def predict(model):\n",
    "  i=0\n",
    "  pred = np.zeros(n, dtype='int')\n",
    "  for batch in test_data:\n",
    "    out = model(batch.to(device))\n",
    "    out = out.detach().cpu().numpy()\n",
    "    pred[i*batch_size:(i+1)*batch_size] = np.argmax(out, axis=1) \n",
    "    i += 1\n",
    "  return pred\n",
    "\n",
    "def find_accuracy(pred):\n",
    "  count = 0\n",
    "  for i in range(n):\n",
    "    if pred[i] != label[3*t_size+n+i]:\n",
    "      count += 1\n",
    "  return 100*(1 - count/n)\n",
    "\n",
    "pred1 = predict(model1)\n",
    "print(\"Accuracy of model1 =\", find_accuracy(pred1))\n",
    "pred2 = predict(model2)\n",
    "print(\"Accuracy of model2 =\", find_accuracy(pred2))\n",
    "pred3 = predict(model3)\n",
    "print(\"Accuracy of model3 =\", find_accuracy(pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final prediction  (Bagging approach)\n",
    "Final label is predicted by taking the average of weighted sum of the predictions made by each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Odf-ohsoTi7I",
    "outputId": "66491cfb-0d5e-429f-d796-e2f46f613437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of overall model = 77.568\n"
     ]
    }
   ],
   "source": [
    "final_label = np.round((count[0]*pred1 + count[1]*pred2 + count[2]*pred3)/3)  \n",
    "print(\"Accuracy of overall model =\", find_accuracy(final_label))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
