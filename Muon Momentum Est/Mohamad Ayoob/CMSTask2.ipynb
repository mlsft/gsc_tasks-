{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CMSTask2.ipynb","provenance":[],"collapsed_sections":["xgDw_4dHx1i_","yTESf8-qaGYn","0SG1jqDOyTfC","yj9W3AnYy478","i8q4ZZ0hBKoh","16hu0Ji5ApNq"],"machine_shape":"hm","authorship_tag":"ABX9TyPf78qArpAkjv5KuM2ZJy8S"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zwptRkSj6kRY","colab_type":"text"},"source":["# Download essential datasets and packages"]},{"cell_type":"code","metadata":{"id":"O6iL_VNT5mYY","colab_type":"code","outputId":"3ec20a0e-aab7-45f0-9b02-77837159c478","executionInfo":{"status":"ok","timestamp":1585479504628,"user_tz":-330,"elapsed":11396,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["!wget https://www.dropbox.com/s/c1pzdacnzhvi6pm/histos_tba.20.npz"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-03-29 10:58:15--  https://www.dropbox.com/s/c1pzdacnzhvi6pm/histos_tba.20.npz\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/c1pzdacnzhvi6pm/histos_tba.20.npz [following]\n","--2020-03-29 10:58:15--  https://www.dropbox.com/s/raw/c1pzdacnzhvi6pm/histos_tba.20.npz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc046f2f056b63425307734023ee.dl.dropboxusercontent.com/cd/0/inline/A00pXiCDDZvpZG2FgQYih9iSVflbBCaWilXy8SukOSczXbT2r8Ovtmksq7ST1LYhMgntGyXHMdoDSvo1m1tN7VQxkkxVZrCNi7WnXMgT3biRYwnAam1xSmq7jlsYkUnemHQ/file# [following]\n","--2020-03-29 10:58:15--  https://uc046f2f056b63425307734023ee.dl.dropboxusercontent.com/cd/0/inline/A00pXiCDDZvpZG2FgQYih9iSVflbBCaWilXy8SukOSczXbT2r8Ovtmksq7ST1LYhMgntGyXHMdoDSvo1m1tN7VQxkkxVZrCNi7WnXMgT3biRYwnAam1xSmq7jlsYkUnemHQ/file\n","Resolving uc046f2f056b63425307734023ee.dl.dropboxusercontent.com (uc046f2f056b63425307734023ee.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n","Connecting to uc046f2f056b63425307734023ee.dl.dropboxusercontent.com (uc046f2f056b63425307734023ee.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n","HTTP request sent, awaiting response... 302 FOUND\n","Location: /cd/0/inline2/A01TnMqX2hD0-VtUradRSl6WdZNagundTOZMaEQ8oXfNT1RWUPy0zSUWChjVPSwjZrfqnsUx-uUAIG_6poxKG8-mTPNK-qWkbTkXg6BoFKTJ_Vl9mwAZyEw7Sw6JSiMUcsv-crc-tmKhf3o7muULb8FAEzd7XgfSj79wZ0BTqU1e0xxJApZMzI6iMakTmbO5o5y04geeC2NL_Bbifkadx0viQQDl0Yrp5jy_i7-xVSi63AHQTQ2plrp19njYaFE94IjkWM9mobitA_waYNie18eAAEyEty3Is0zgKp0z_DKCBFip6sW_ukQzH5xy6-ajiTE_WokclfzkY6IzohaKJhigZZBrzvvUXBFGDqFc-9jedw/file [following]\n","--2020-03-29 10:58:16--  https://uc046f2f056b63425307734023ee.dl.dropboxusercontent.com/cd/0/inline2/A01TnMqX2hD0-VtUradRSl6WdZNagundTOZMaEQ8oXfNT1RWUPy0zSUWChjVPSwjZrfqnsUx-uUAIG_6poxKG8-mTPNK-qWkbTkXg6BoFKTJ_Vl9mwAZyEw7Sw6JSiMUcsv-crc-tmKhf3o7muULb8FAEzd7XgfSj79wZ0BTqU1e0xxJApZMzI6iMakTmbO5o5y04geeC2NL_Bbifkadx0viQQDl0Yrp5jy_i7-xVSi63AHQTQ2plrp19njYaFE94IjkWM9mobitA_waYNie18eAAEyEty3Is0zgKp0z_DKCBFip6sW_ukQzH5xy6-ajiTE_WokclfzkY6IzohaKJhigZZBrzvvUXBFGDqFc-9jedw/file\n","Reusing existing connection to uc046f2f056b63425307734023ee.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 223440772 (213M) [application/octet-stream]\n","Saving to: ‘histos_tba.20.npz.1’\n","\n","histos_tba.20.npz.1 100%[===================>] 213.09M  32.0MB/s    in 6.4s    \n","\n","2020-03-29 10:58:23 (33.2 MB/s) - ‘histos_tba.20.npz.1’ saved [223440772/223440772]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I6yYMwbC6sYe","colab_type":"text"},"source":["Python module dependencies"]},{"cell_type":"code","metadata":{"id":"H-Py_Air9aKS","colab_type":"code","colab":{}},"source":["# %tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-KTxWCW6OHO","colab_type":"code","outputId":"b0c583e7-1b57-470f-c3b8-422fc0416b77","executionInfo":{"status":"ok","timestamp":1585480252618,"user_tz":-330,"elapsed":2850,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["import numpy as np\n","from scipy import stats\n","from collections import Counter\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import sys\n","import re \n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"VMCG9IzV7ZGQ","colab_type":"text"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"m-MIUQa-6P2t","colab_type":"code","colab":{}},"source":["data = np.load('histos_tba.20.npz')\n","variables = data['variables']\n","parameters = data['parameters']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKRd7YelCtIE","colab_type":"text"},"source":["# Data Preparation"]},{"cell_type":"code","metadata":{"id":"SvDiXg3fCvTv","colab_type":"code","colab":{}},"source":["variables = variables[:,:84]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9PyrxdUVKEQS","colab_type":"text"},"source":["parameter array 0th index contains charge/momentum, to get the momentum we have to get the negative reciprocal of the charge / momentum"]},{"cell_type":"code","metadata":{"id":"1Vb4u5y9J1tj","colab_type":"code","outputId":"bfe62f8b-db91-45e5-b62a-572de27e0374","executionInfo":{"status":"ok","timestamp":1585480258485,"user_tz":-330,"elapsed":4089,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["transverse_momentum = -1 / np.delete(parameters, [1,2], 1) \n","stats.describe(transverse_momentum)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DescribeResult(nobs=3272341, minmax=(array([-6989.3447], dtype=float32), array([6989.252], dtype=float32)), mean=array([0.03644617], dtype=float32), variance=array([16305.736], dtype=float32), skewness=array([-0.50260574], dtype=float32), kurtosis=array([991.76306], dtype=float32))"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"XCnZ5iX_OeVE","colab_type":"code","colab":{}},"source":["dataset = np.hstack((variables,transverse_momentum))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__4HyBdhcLJ3","colab_type":"code","outputId":"a66d44f6-12d0-47cd-8961-994f6c7de412","executionInfo":{"status":"ok","timestamp":1585480281276,"user_tz":-330,"elapsed":23475,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["cleaned_dataset = []\n","for e in dataset:\n","  if e[84] > 0.0 and e[84] <= 10.0:\n","    e[84] = 1\n","    cleaned_dataset.append(e)\n","  elif e[84] > 10.0 and e[84] <= 30.0:\n","    e[84] = 2\n","    cleaned_dataset.append(e)\n","  elif e[84] > 30.0 and e[84] <= 100.0:\n","    e[84] = 3\n","    cleaned_dataset.append(e)\n","  elif e[84] > 100.0:\n","    e[84] = 4\n","    cleaned_dataset.append(e)\n","  else:\n","    e[84] = 5\n","    cleaned_dataset.append(e)\n","\n","cleaned_dataset = np.asarray(cleaned_dataset)\n","cleaned_dataset"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3.190e+03,       nan, 3.546e+03, ..., 0.000e+00, 0.000e+00,\n","        1.000e+00],\n","       [2.865e+03,       nan, 2.622e+03, ..., 0.000e+00, 0.000e+00,\n","        5.000e+00],\n","       [      nan, 1.415e+03, 1.455e+03, ..., 1.000e+00, 1.000e+00,\n","        1.000e+00],\n","       ...,\n","       [4.054e+03,       nan, 4.016e+03, ..., 0.000e+00, 0.000e+00,\n","        5.000e+00],\n","       [      nan, 3.464e+03, 3.439e+03, ..., 1.000e+00, 1.000e+00,\n","        4.000e+00],\n","       [8.360e+02,       nan, 2.960e+02, ..., 0.000e+00, 1.000e+00,\n","        5.000e+00]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"SlrNOsGgd7_k","colab_type":"code","outputId":"0e785455-653a-49d1-c1cf-2fc6660a2255","executionInfo":{"status":"ok","timestamp":1585479538872,"user_tz":-330,"elapsed":45577,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["stats.describe(cleaned_dataset.T[84])"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DescribeResult(nobs=3272341, minmax=(1.0, 5.0), mean=3.1621292, variance=3.6044843, skewness=-0.13364042341709137, kurtosis=-1.900529585828819)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"H3oP6XhgwfB4","colab_type":"code","colab":{}},"source":["X = cleaned_dataset[:,0:84]\n","X = np.nan_to_num(X)\n","y = cleaned_dataset[:,84]\n","y = y.astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgDw_4dHx1i_","colab_type":"text"},"source":["# Downsampling data and creating images\n","\n","Images are made in the shape of 12 X 7\n","\n","this conforms to the detector planes and the number of features"]},{"cell_type":"code","metadata":{"id":"2IZBCoOIxC9W","colab_type":"code","outputId":"f7edc25f-b484-4083-fb12-06d9fdd1737c","executionInfo":{"status":"ok","timestamp":1585480290874,"user_tz":-330,"elapsed":3451,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print('Original dataset shape %s' % Counter(y))\n","\n","rus = RandomUnderSampler(random_state=42)\n","X_res, y_res = rus.fit_resample(X, y)\n","print('Resampled dataset shape %s' % Counter(y_res))\n","X_res = X_res.reshape(185350,7,12)\n","y_res = y_res.reshape(-1)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Original dataset shape Counter({5: 1635011, 1: 1263726, 2: 249098, 3: 87436, 4: 37070})\n","Resampled dataset shape Counter({1: 37070, 2: 37070, 3: 37070, 4: 37070, 5: 37070})\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"0SG1jqDOyTfC","colab_type":"text"},"source":["# Train Test Split"]},{"cell_type":"code","metadata":{"id":"8NagAEcyyWFN","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kUe93pv3y_t6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"fdb0b674-d8e9-4fa3-c86d-ae6b0710e77d","executionInfo":{"status":"ok","timestamp":1585480834585,"user_tz":-330,"elapsed":1163,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils import data\n","\n","# Check Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define Hyper-parameters \n","input_size = 84\n","hidden_size = 500\n","hidden_size_l2 = 300\n","num_classes = 6\n","num_epochs = 30\n","batch_size = 100\n","learning_rate = 0.001\n","\n","tensor_x = torch.Tensor(X_train) # transform to train torch tensor\n","tensor_y = torch.Tensor(y_train)\n","tensor_y = torch.tensor(tensor_y,dtype=torch.long, device=device)\n","\n","test_tensor_x = torch.Tensor(X_test) # transform to test torch tensor\n","test_tensor_y = torch.Tensor(y_test)\n","test_tensor_y = torch.tensor(test_tensor_y,dtype=torch.long, device=device)\n","\n","train_dataset = data.TensorDataset(tensor_x,tensor_y)\n","train_loader = data.DataLoader(train_dataset,batch_size=batch_size)\n","\n","test_dataset = data.TensorDataset(test_tensor_x,test_tensor_y)\n","test_loader = data.DataLoader(test_dataset,batch_size=batch_size)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"yj9W3AnYy478","colab_type":"text"},"source":["# Fully Connected Neural Network\n","\n","The parameters were made on looking at the euclidean data spread of the dataset."]},{"cell_type":"code","metadata":{"id":"3IH5raxX0aTv","colab_type":"code","colab":{}},"source":["# Fully connected neural network\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes):\n","        super(NeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size) \n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, hidden_size_l2)\n","        self.relu = nn.ReLU()\n","        self.fc3 = nn.Linear(hidden_size_l2, num_classes)\n","    \n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        out = self.relu(out)\n","        out = self.fc3(out)\n","        return out\n","\n","model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmjBvYo7fZG1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b41e4e94-0ecf-49c6-bda4-ba3f0ad4cd27","executionInfo":{"status":"ok","timestamp":1585481084439,"user_tz":-330,"elapsed":246481,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}}},"source":["# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # Move tensors to the configured device\n","        images = images.reshape(-1, 7*12).to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backprpagation and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","# In the test phase, don't need to compute gradients (for memory efficiency)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 7*12).to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Epoch [1/30], Step [100/1298], Loss: 4.0190\n","Epoch [1/30], Step [200/1298], Loss: 3.1071\n","Epoch [1/30], Step [300/1298], Loss: 2.2777\n","Epoch [1/30], Step [400/1298], Loss: 1.7598\n","Epoch [1/30], Step [500/1298], Loss: 2.3973\n","Epoch [1/30], Step [600/1298], Loss: 1.3483\n","Epoch [1/30], Step [700/1298], Loss: 1.2184\n","Epoch [1/30], Step [800/1298], Loss: 1.1770\n","Epoch [1/30], Step [900/1298], Loss: 1.1317\n","Epoch [1/30], Step [1000/1298], Loss: 1.0188\n","Epoch [1/30], Step [1100/1298], Loss: 1.1775\n","Epoch [1/30], Step [1200/1298], Loss: 0.8917\n","Epoch [2/30], Step [100/1298], Loss: 1.0059\n","Epoch [2/30], Step [200/1298], Loss: 1.1653\n","Epoch [2/30], Step [300/1298], Loss: 0.9331\n","Epoch [2/30], Step [400/1298], Loss: 1.0214\n","Epoch [2/30], Step [500/1298], Loss: 0.9558\n","Epoch [2/30], Step [600/1298], Loss: 0.9886\n","Epoch [2/30], Step [700/1298], Loss: 0.9772\n","Epoch [2/30], Step [800/1298], Loss: 1.2125\n","Epoch [2/30], Step [900/1298], Loss: 0.7978\n","Epoch [2/30], Step [1000/1298], Loss: 0.9634\n","Epoch [2/30], Step [1100/1298], Loss: 0.8452\n","Epoch [2/30], Step [1200/1298], Loss: 0.8404\n","Epoch [3/30], Step [100/1298], Loss: 0.9684\n","Epoch [3/30], Step [200/1298], Loss: 0.9793\n","Epoch [3/30], Step [300/1298], Loss: 0.9190\n","Epoch [3/30], Step [400/1298], Loss: 0.9417\n","Epoch [3/30], Step [500/1298], Loss: 0.9805\n","Epoch [3/30], Step [600/1298], Loss: 0.9652\n","Epoch [3/30], Step [700/1298], Loss: 0.9484\n","Epoch [3/30], Step [800/1298], Loss: 0.9713\n","Epoch [3/30], Step [900/1298], Loss: 0.8675\n","Epoch [3/30], Step [1000/1298], Loss: 0.8525\n","Epoch [3/30], Step [1100/1298], Loss: 0.8578\n","Epoch [3/30], Step [1200/1298], Loss: 0.7848\n","Epoch [4/30], Step [100/1298], Loss: 0.8771\n","Epoch [4/30], Step [200/1298], Loss: 0.9480\n","Epoch [4/30], Step [300/1298], Loss: 0.8531\n","Epoch [4/30], Step [400/1298], Loss: 0.8327\n","Epoch [4/30], Step [500/1298], Loss: 1.1484\n","Epoch [4/30], Step [600/1298], Loss: 1.0056\n","Epoch [4/30], Step [700/1298], Loss: 0.8870\n","Epoch [4/30], Step [800/1298], Loss: 0.9997\n","Epoch [4/30], Step [900/1298], Loss: 0.7734\n","Epoch [4/30], Step [1000/1298], Loss: 0.8731\n","Epoch [4/30], Step [1100/1298], Loss: 0.8445\n","Epoch [4/30], Step [1200/1298], Loss: 0.7716\n","Epoch [5/30], Step [100/1298], Loss: 0.8809\n","Epoch [5/30], Step [200/1298], Loss: 0.9021\n","Epoch [5/30], Step [300/1298], Loss: 0.9317\n","Epoch [5/30], Step [400/1298], Loss: 0.8802\n","Epoch [5/30], Step [500/1298], Loss: 0.9567\n","Epoch [5/30], Step [600/1298], Loss: 1.0508\n","Epoch [5/30], Step [700/1298], Loss: 0.9314\n","Epoch [5/30], Step [800/1298], Loss: 1.1253\n","Epoch [5/30], Step [900/1298], Loss: 0.8019\n","Epoch [5/30], Step [1000/1298], Loss: 0.8235\n","Epoch [5/30], Step [1100/1298], Loss: 0.7985\n","Epoch [5/30], Step [1200/1298], Loss: 0.7149\n","Epoch [6/30], Step [100/1298], Loss: 0.8289\n","Epoch [6/30], Step [200/1298], Loss: 0.9014\n","Epoch [6/30], Step [300/1298], Loss: 0.8912\n","Epoch [6/30], Step [400/1298], Loss: 0.9108\n","Epoch [6/30], Step [500/1298], Loss: 0.9382\n","Epoch [6/30], Step [600/1298], Loss: 0.8728\n","Epoch [6/30], Step [700/1298], Loss: 0.9001\n","Epoch [6/30], Step [800/1298], Loss: 1.0600\n","Epoch [6/30], Step [900/1298], Loss: 0.7865\n","Epoch [6/30], Step [1000/1298], Loss: 0.9684\n","Epoch [6/30], Step [1100/1298], Loss: 0.8581\n","Epoch [6/30], Step [1200/1298], Loss: 0.7118\n","Epoch [7/30], Step [100/1298], Loss: 0.8307\n","Epoch [7/30], Step [200/1298], Loss: 0.8052\n","Epoch [7/30], Step [300/1298], Loss: 0.8360\n","Epoch [7/30], Step [400/1298], Loss: 0.8269\n","Epoch [7/30], Step [500/1298], Loss: 0.9201\n","Epoch [7/30], Step [600/1298], Loss: 0.8573\n","Epoch [7/30], Step [700/1298], Loss: 0.8942\n","Epoch [7/30], Step [800/1298], Loss: 1.0585\n","Epoch [7/30], Step [900/1298], Loss: 0.7609\n","Epoch [7/30], Step [1000/1298], Loss: 0.8207\n","Epoch [7/30], Step [1100/1298], Loss: 0.8084\n","Epoch [7/30], Step [1200/1298], Loss: 0.7727\n","Epoch [8/30], Step [100/1298], Loss: 0.8358\n","Epoch [8/30], Step [200/1298], Loss: 0.8111\n","Epoch [8/30], Step [300/1298], Loss: 0.7762\n","Epoch [8/30], Step [400/1298], Loss: 0.8247\n","Epoch [8/30], Step [500/1298], Loss: 0.8903\n","Epoch [8/30], Step [600/1298], Loss: 0.8685\n","Epoch [8/30], Step [700/1298], Loss: 0.8451\n","Epoch [8/30], Step [800/1298], Loss: 1.0564\n","Epoch [8/30], Step [900/1298], Loss: 0.7289\n","Epoch [8/30], Step [1000/1298], Loss: 0.8071\n","Epoch [8/30], Step [1100/1298], Loss: 0.7997\n","Epoch [8/30], Step [1200/1298], Loss: 0.6792\n","Epoch [9/30], Step [100/1298], Loss: 0.9161\n","Epoch [9/30], Step [200/1298], Loss: 0.8146\n","Epoch [9/30], Step [300/1298], Loss: 0.8059\n","Epoch [9/30], Step [400/1298], Loss: 0.8503\n","Epoch [9/30], Step [500/1298], Loss: 0.8756\n","Epoch [9/30], Step [600/1298], Loss: 0.8180\n","Epoch [9/30], Step [700/1298], Loss: 0.8195\n","Epoch [9/30], Step [800/1298], Loss: 0.8978\n","Epoch [9/30], Step [900/1298], Loss: 0.7139\n","Epoch [9/30], Step [1000/1298], Loss: 0.7522\n","Epoch [9/30], Step [1100/1298], Loss: 0.7963\n","Epoch [9/30], Step [1200/1298], Loss: 0.6757\n","Epoch [10/30], Step [100/1298], Loss: 0.8050\n","Epoch [10/30], Step [200/1298], Loss: 0.7258\n","Epoch [10/30], Step [300/1298], Loss: 0.7654\n","Epoch [10/30], Step [400/1298], Loss: 0.8010\n","Epoch [10/30], Step [500/1298], Loss: 0.9099\n","Epoch [10/30], Step [600/1298], Loss: 0.7758\n","Epoch [10/30], Step [700/1298], Loss: 0.8582\n","Epoch [10/30], Step [800/1298], Loss: 0.9612\n","Epoch [10/30], Step [900/1298], Loss: 0.6691\n","Epoch [10/30], Step [1000/1298], Loss: 0.7778\n","Epoch [10/30], Step [1100/1298], Loss: 0.7612\n","Epoch [10/30], Step [1200/1298], Loss: 0.6778\n","Epoch [11/30], Step [100/1298], Loss: 0.8499\n","Epoch [11/30], Step [200/1298], Loss: 0.6725\n","Epoch [11/30], Step [300/1298], Loss: 0.7785\n","Epoch [11/30], Step [400/1298], Loss: 0.8721\n","Epoch [11/30], Step [500/1298], Loss: 0.9454\n","Epoch [11/30], Step [600/1298], Loss: 0.8296\n","Epoch [11/30], Step [700/1298], Loss: 0.8260\n","Epoch [11/30], Step [800/1298], Loss: 0.9057\n","Epoch [11/30], Step [900/1298], Loss: 0.7426\n","Epoch [11/30], Step [1000/1298], Loss: 0.7660\n","Epoch [11/30], Step [1100/1298], Loss: 0.7417\n","Epoch [11/30], Step [1200/1298], Loss: 0.6567\n","Epoch [12/30], Step [100/1298], Loss: 0.8674\n","Epoch [12/30], Step [200/1298], Loss: 0.7033\n","Epoch [12/30], Step [300/1298], Loss: 0.8212\n","Epoch [12/30], Step [400/1298], Loss: 0.8249\n","Epoch [12/30], Step [500/1298], Loss: 0.9481\n","Epoch [12/30], Step [600/1298], Loss: 0.8399\n","Epoch [12/30], Step [700/1298], Loss: 0.8153\n","Epoch [12/30], Step [800/1298], Loss: 0.9779\n","Epoch [12/30], Step [900/1298], Loss: 0.6270\n","Epoch [12/30], Step [1000/1298], Loss: 0.6867\n","Epoch [12/30], Step [1100/1298], Loss: 0.7655\n","Epoch [12/30], Step [1200/1298], Loss: 0.6469\n","Epoch [13/30], Step [100/1298], Loss: 0.8125\n","Epoch [13/30], Step [200/1298], Loss: 0.6753\n","Epoch [13/30], Step [300/1298], Loss: 0.7924\n","Epoch [13/30], Step [400/1298], Loss: 0.7862\n","Epoch [13/30], Step [500/1298], Loss: 0.9635\n","Epoch [13/30], Step [600/1298], Loss: 0.7955\n","Epoch [13/30], Step [700/1298], Loss: 0.7558\n","Epoch [13/30], Step [800/1298], Loss: 0.8050\n","Epoch [13/30], Step [900/1298], Loss: 0.6757\n","Epoch [13/30], Step [1000/1298], Loss: 0.6554\n","Epoch [13/30], Step [1100/1298], Loss: 0.7216\n","Epoch [13/30], Step [1200/1298], Loss: 0.6598\n","Epoch [14/30], Step [100/1298], Loss: 0.7855\n","Epoch [14/30], Step [200/1298], Loss: 0.6750\n","Epoch [14/30], Step [300/1298], Loss: 0.7201\n","Epoch [14/30], Step [400/1298], Loss: 0.7910\n","Epoch [14/30], Step [500/1298], Loss: 0.8912\n","Epoch [14/30], Step [600/1298], Loss: 0.7804\n","Epoch [14/30], Step [700/1298], Loss: 0.8078\n","Epoch [14/30], Step [800/1298], Loss: 0.8275\n","Epoch [14/30], Step [900/1298], Loss: 0.6753\n","Epoch [14/30], Step [1000/1298], Loss: 0.6649\n","Epoch [14/30], Step [1100/1298], Loss: 0.7166\n","Epoch [14/30], Step [1200/1298], Loss: 0.5957\n","Epoch [15/30], Step [100/1298], Loss: 0.8036\n","Epoch [15/30], Step [200/1298], Loss: 0.7193\n","Epoch [15/30], Step [300/1298], Loss: 0.8595\n","Epoch [15/30], Step [400/1298], Loss: 0.8027\n","Epoch [15/30], Step [500/1298], Loss: 0.9380\n","Epoch [15/30], Step [600/1298], Loss: 0.8682\n","Epoch [15/30], Step [700/1298], Loss: 0.7738\n","Epoch [15/30], Step [800/1298], Loss: 0.7911\n","Epoch [15/30], Step [900/1298], Loss: 0.6730\n","Epoch [15/30], Step [1000/1298], Loss: 0.6949\n","Epoch [15/30], Step [1100/1298], Loss: 0.7609\n","Epoch [15/30], Step [1200/1298], Loss: 0.5692\n","Epoch [16/30], Step [100/1298], Loss: 0.7950\n","Epoch [16/30], Step [200/1298], Loss: 0.6159\n","Epoch [16/30], Step [300/1298], Loss: 0.7308\n","Epoch [16/30], Step [400/1298], Loss: 0.7447\n","Epoch [16/30], Step [500/1298], Loss: 0.9097\n","Epoch [16/30], Step [600/1298], Loss: 0.7992\n","Epoch [16/30], Step [700/1298], Loss: 0.7653\n","Epoch [16/30], Step [800/1298], Loss: 0.8194\n","Epoch [16/30], Step [900/1298], Loss: 0.6014\n","Epoch [16/30], Step [1000/1298], Loss: 0.6823\n","Epoch [16/30], Step [1100/1298], Loss: 0.6982\n","Epoch [16/30], Step [1200/1298], Loss: 0.5744\n","Epoch [17/30], Step [100/1298], Loss: 0.7459\n","Epoch [17/30], Step [200/1298], Loss: 0.6261\n","Epoch [17/30], Step [300/1298], Loss: 0.7464\n","Epoch [17/30], Step [400/1298], Loss: 0.7837\n","Epoch [17/30], Step [500/1298], Loss: 0.9104\n","Epoch [17/30], Step [600/1298], Loss: 0.7471\n","Epoch [17/30], Step [700/1298], Loss: 0.7994\n","Epoch [17/30], Step [800/1298], Loss: 0.8275\n","Epoch [17/30], Step [900/1298], Loss: 0.6854\n","Epoch [17/30], Step [1000/1298], Loss: 0.6174\n","Epoch [17/30], Step [1100/1298], Loss: 0.6940\n","Epoch [17/30], Step [1200/1298], Loss: 0.5696\n","Epoch [18/30], Step [100/1298], Loss: 0.6803\n","Epoch [18/30], Step [200/1298], Loss: 0.6079\n","Epoch [18/30], Step [300/1298], Loss: 0.7012\n","Epoch [18/30], Step [400/1298], Loss: 0.7848\n","Epoch [18/30], Step [500/1298], Loss: 0.7282\n","Epoch [18/30], Step [600/1298], Loss: 0.7731\n","Epoch [18/30], Step [700/1298], Loss: 0.8010\n","Epoch [18/30], Step [800/1298], Loss: 0.8508\n","Epoch [18/30], Step [900/1298], Loss: 0.6912\n","Epoch [18/30], Step [1000/1298], Loss: 0.6687\n","Epoch [18/30], Step [1100/1298], Loss: 0.8607\n","Epoch [18/30], Step [1200/1298], Loss: 0.6103\n","Epoch [19/30], Step [100/1298], Loss: 0.6629\n","Epoch [19/30], Step [200/1298], Loss: 0.5787\n","Epoch [19/30], Step [300/1298], Loss: 0.7719\n","Epoch [19/30], Step [400/1298], Loss: 0.7324\n","Epoch [19/30], Step [500/1298], Loss: 0.9200\n","Epoch [19/30], Step [600/1298], Loss: 0.7816\n","Epoch [19/30], Step [700/1298], Loss: 0.7266\n","Epoch [19/30], Step [800/1298], Loss: 0.8128\n","Epoch [19/30], Step [900/1298], Loss: 0.7546\n","Epoch [19/30], Step [1000/1298], Loss: 0.6538\n","Epoch [19/30], Step [1100/1298], Loss: 0.7458\n","Epoch [19/30], Step [1200/1298], Loss: 0.6103\n","Epoch [20/30], Step [100/1298], Loss: 0.6298\n","Epoch [20/30], Step [200/1298], Loss: 0.5928\n","Epoch [20/30], Step [300/1298], Loss: 0.8233\n","Epoch [20/30], Step [400/1298], Loss: 0.7651\n","Epoch [20/30], Step [500/1298], Loss: 0.9126\n","Epoch [20/30], Step [600/1298], Loss: 0.7706\n","Epoch [20/30], Step [700/1298], Loss: 0.7477\n","Epoch [20/30], Step [800/1298], Loss: 0.7923\n","Epoch [20/30], Step [900/1298], Loss: 0.6353\n","Epoch [20/30], Step [1000/1298], Loss: 0.6206\n","Epoch [20/30], Step [1100/1298], Loss: 0.7018\n","Epoch [20/30], Step [1200/1298], Loss: 0.5631\n","Epoch [21/30], Step [100/1298], Loss: 0.6893\n","Epoch [21/30], Step [200/1298], Loss: 0.5644\n","Epoch [21/30], Step [300/1298], Loss: 0.7037\n","Epoch [21/30], Step [400/1298], Loss: 0.6974\n","Epoch [21/30], Step [500/1298], Loss: 0.7283\n","Epoch [21/30], Step [600/1298], Loss: 0.6553\n","Epoch [21/30], Step [700/1298], Loss: 0.7829\n","Epoch [21/30], Step [800/1298], Loss: 0.8476\n","Epoch [21/30], Step [900/1298], Loss: 0.6712\n","Epoch [21/30], Step [1000/1298], Loss: 0.6231\n","Epoch [21/30], Step [1100/1298], Loss: 0.7047\n","Epoch [21/30], Step [1200/1298], Loss: 0.5028\n","Epoch [22/30], Step [100/1298], Loss: 0.6671\n","Epoch [22/30], Step [200/1298], Loss: 0.5653\n","Epoch [22/30], Step [300/1298], Loss: 0.7463\n","Epoch [22/30], Step [400/1298], Loss: 0.7643\n","Epoch [22/30], Step [500/1298], Loss: 0.7854\n","Epoch [22/30], Step [600/1298], Loss: 0.6715\n","Epoch [22/30], Step [700/1298], Loss: 0.7321\n","Epoch [22/30], Step [800/1298], Loss: 0.7590\n","Epoch [22/30], Step [900/1298], Loss: 0.6744\n","Epoch [22/30], Step [1000/1298], Loss: 0.6066\n","Epoch [22/30], Step [1100/1298], Loss: 0.7321\n","Epoch [22/30], Step [1200/1298], Loss: 0.5916\n","Epoch [23/30], Step [100/1298], Loss: 0.7634\n","Epoch [23/30], Step [200/1298], Loss: 0.6229\n","Epoch [23/30], Step [300/1298], Loss: 0.7886\n","Epoch [23/30], Step [400/1298], Loss: 0.7443\n","Epoch [23/30], Step [500/1298], Loss: 0.7294\n","Epoch [23/30], Step [600/1298], Loss: 0.6843\n","Epoch [23/30], Step [700/1298], Loss: 0.6584\n","Epoch [23/30], Step [800/1298], Loss: 0.7767\n","Epoch [23/30], Step [900/1298], Loss: 0.6136\n","Epoch [23/30], Step [1000/1298], Loss: 0.6192\n","Epoch [23/30], Step [1100/1298], Loss: 0.7362\n","Epoch [23/30], Step [1200/1298], Loss: 0.5158\n","Epoch [24/30], Step [100/1298], Loss: 0.7114\n","Epoch [24/30], Step [200/1298], Loss: 0.5935\n","Epoch [24/30], Step [300/1298], Loss: 0.8524\n","Epoch [24/30], Step [400/1298], Loss: 0.7589\n","Epoch [24/30], Step [500/1298], Loss: 0.8308\n","Epoch [24/30], Step [600/1298], Loss: 0.6616\n","Epoch [24/30], Step [700/1298], Loss: 0.6611\n","Epoch [24/30], Step [800/1298], Loss: 0.7362\n","Epoch [24/30], Step [900/1298], Loss: 0.6342\n","Epoch [24/30], Step [1000/1298], Loss: 0.6392\n","Epoch [24/30], Step [1100/1298], Loss: 0.6782\n","Epoch [24/30], Step [1200/1298], Loss: 0.5808\n","Epoch [25/30], Step [100/1298], Loss: 0.6527\n","Epoch [25/30], Step [200/1298], Loss: 0.5647\n","Epoch [25/30], Step [300/1298], Loss: 0.7854\n","Epoch [25/30], Step [400/1298], Loss: 0.8076\n","Epoch [25/30], Step [500/1298], Loss: 0.7474\n","Epoch [25/30], Step [600/1298], Loss: 0.7052\n","Epoch [25/30], Step [700/1298], Loss: 0.7272\n","Epoch [25/30], Step [800/1298], Loss: 0.6964\n","Epoch [25/30], Step [900/1298], Loss: 0.5993\n","Epoch [25/30], Step [1000/1298], Loss: 0.6746\n","Epoch [25/30], Step [1100/1298], Loss: 0.6838\n","Epoch [25/30], Step [1200/1298], Loss: 0.5399\n","Epoch [26/30], Step [100/1298], Loss: 0.6594\n","Epoch [26/30], Step [200/1298], Loss: 0.6626\n","Epoch [26/30], Step [300/1298], Loss: 0.6982\n","Epoch [26/30], Step [400/1298], Loss: 0.7454\n","Epoch [26/30], Step [500/1298], Loss: 0.8672\n","Epoch [26/30], Step [600/1298], Loss: 0.6614\n","Epoch [26/30], Step [700/1298], Loss: 0.6726\n","Epoch [26/30], Step [800/1298], Loss: 0.6939\n","Epoch [26/30], Step [900/1298], Loss: 0.5905\n","Epoch [26/30], Step [1000/1298], Loss: 0.6135\n","Epoch [26/30], Step [1100/1298], Loss: 0.6484\n","Epoch [26/30], Step [1200/1298], Loss: 0.5499\n","Epoch [27/30], Step [100/1298], Loss: 0.7327\n","Epoch [27/30], Step [200/1298], Loss: 0.6127\n","Epoch [27/30], Step [300/1298], Loss: 0.7742\n","Epoch [27/30], Step [400/1298], Loss: 0.7623\n","Epoch [27/30], Step [500/1298], Loss: 0.9810\n","Epoch [27/30], Step [600/1298], Loss: 0.7496\n","Epoch [27/30], Step [700/1298], Loss: 0.6197\n","Epoch [27/30], Step [800/1298], Loss: 0.6914\n","Epoch [27/30], Step [900/1298], Loss: 0.5655\n","Epoch [27/30], Step [1000/1298], Loss: 0.5939\n","Epoch [27/30], Step [1100/1298], Loss: 0.6712\n","Epoch [27/30], Step [1200/1298], Loss: 0.5429\n","Epoch [28/30], Step [100/1298], Loss: 0.6794\n","Epoch [28/30], Step [200/1298], Loss: 0.5918\n","Epoch [28/30], Step [300/1298], Loss: 0.7189\n","Epoch [28/30], Step [400/1298], Loss: 0.7683\n","Epoch [28/30], Step [500/1298], Loss: 0.7195\n","Epoch [28/30], Step [600/1298], Loss: 0.7635\n","Epoch [28/30], Step [700/1298], Loss: 0.7047\n","Epoch [28/30], Step [800/1298], Loss: 0.7701\n","Epoch [28/30], Step [900/1298], Loss: 0.5964\n","Epoch [28/30], Step [1000/1298], Loss: 0.6123\n","Epoch [28/30], Step [1100/1298], Loss: 0.6451\n","Epoch [28/30], Step [1200/1298], Loss: 0.5482\n","Epoch [29/30], Step [100/1298], Loss: 0.7290\n","Epoch [29/30], Step [200/1298], Loss: 0.6068\n","Epoch [29/30], Step [300/1298], Loss: 0.6846\n","Epoch [29/30], Step [400/1298], Loss: 0.7410\n","Epoch [29/30], Step [500/1298], Loss: 0.7461\n","Epoch [29/30], Step [600/1298], Loss: 0.7313\n","Epoch [29/30], Step [700/1298], Loss: 0.7043\n","Epoch [29/30], Step [800/1298], Loss: 0.7179\n","Epoch [29/30], Step [900/1298], Loss: 0.5522\n","Epoch [29/30], Step [1000/1298], Loss: 0.5871\n","Epoch [29/30], Step [1100/1298], Loss: 0.6690\n","Epoch [29/30], Step [1200/1298], Loss: 0.5354\n","Epoch [30/30], Step [100/1298], Loss: 0.7558\n","Epoch [30/30], Step [200/1298], Loss: 0.6745\n","Epoch [30/30], Step [300/1298], Loss: 0.7203\n","Epoch [30/30], Step [400/1298], Loss: 0.7481\n","Epoch [30/30], Step [500/1298], Loss: 0.8609\n","Epoch [30/30], Step [600/1298], Loss: 0.7384\n","Epoch [30/30], Step [700/1298], Loss: 0.6307\n","Epoch [30/30], Step [800/1298], Loss: 0.6547\n","Epoch [30/30], Step [900/1298], Loss: 0.5921\n","Epoch [30/30], Step [1000/1298], Loss: 0.5660\n","Epoch [30/30], Step [1100/1298], Loss: 0.6440\n","Epoch [30/30], Step [1200/1298], Loss: 0.5483\n","Accuracy of the network on the test images: 70.9360669004586 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zwCuIUxHtXHF","colab_type":"text"},"source":["# 2D Convolutional Neural Network \n","\n","epoch to epoch the CNN gives better accuracy.\n","\n","The parameters of the models were defined by theoretical knowledge on CNNs such as strides and kernel size. I made them small since the the image size were training is small as well. ( 12 X 7)"]},{"cell_type":"code","metadata":{"id":"cNv-IJw4vSdH","colab_type":"code","colab":{}},"source":["class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=2, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=2, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.drop_out = nn.Dropout()\n","        self.fc1 = nn.Linear(384, 500)\n","        self.fc2 = nn.Linear(500, 6)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.drop_out(out)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5DyzpYlovuon","colab_type":"code","colab":{}},"source":["modelConv = ConvNet()\n","modelConv.to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(modelConv.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kiCipAuv0JS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d952ac8a-e9d3-4991-a388-388271ae728d","executionInfo":{"status":"ok","timestamp":1585480649386,"user_tz":-330,"elapsed":208094,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}}},"source":["num_epochs = 10\n","# Train the model\n","total_step = len(train_loader)\n","loss_list = []\n","acc_list = []\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        if len(images) != 100:\n","          continue\n","        labels = labels.to(device)\n","        # Run the forward pass\n","        outputs = modelConv(images.reshape(100,1,7,12).to(device))\n","        loss = criterion(outputs, labels)\n","        loss_list.append(loss.item())\n","\n","        # Backprop and perform Adam optimisation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track the accuracy\n","        total = labels.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        correct = (predicted == labels).sum().item()\n","        acc_list.append(correct / total)\n","\n","        if (i + 1) % 100 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n","                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n","                          (correct / total) * 100))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Epoch [1/10], Step [100/1298], Loss: 1.1642, Accuracy: 52.00%\n","Epoch [1/10], Step [200/1298], Loss: 0.9481, Accuracy: 60.00%\n","Epoch [1/10], Step [300/1298], Loss: 1.0917, Accuracy: 53.00%\n","Epoch [1/10], Step [400/1298], Loss: 1.0306, Accuracy: 56.00%\n","Epoch [1/10], Step [500/1298], Loss: 1.0861, Accuracy: 52.00%\n","Epoch [1/10], Step [600/1298], Loss: 1.0128, Accuracy: 55.00%\n","Epoch [1/10], Step [700/1298], Loss: 1.1650, Accuracy: 44.00%\n","Epoch [1/10], Step [800/1298], Loss: 1.0874, Accuracy: 57.00%\n","Epoch [1/10], Step [900/1298], Loss: 0.8103, Accuracy: 69.00%\n","Epoch [1/10], Step [1000/1298], Loss: 0.9047, Accuracy: 58.00%\n","Epoch [1/10], Step [1100/1298], Loss: 1.0228, Accuracy: 57.00%\n","Epoch [1/10], Step [1200/1298], Loss: 1.0219, Accuracy: 57.00%\n","Epoch [2/10], Step [100/1298], Loss: 1.0456, Accuracy: 52.00%\n","Epoch [2/10], Step [200/1298], Loss: 0.9450, Accuracy: 61.00%\n","Epoch [2/10], Step [300/1298], Loss: 1.1386, Accuracy: 50.00%\n","Epoch [2/10], Step [400/1298], Loss: 0.9722, Accuracy: 60.00%\n","Epoch [2/10], Step [500/1298], Loss: 1.1411, Accuracy: 54.00%\n","Epoch [2/10], Step [600/1298], Loss: 1.0704, Accuracy: 54.00%\n","Epoch [2/10], Step [700/1298], Loss: 1.2340, Accuracy: 50.00%\n","Epoch [2/10], Step [800/1298], Loss: 1.0118, Accuracy: 60.00%\n","Epoch [2/10], Step [900/1298], Loss: 0.8401, Accuracy: 70.00%\n","Epoch [2/10], Step [1000/1298], Loss: 0.8945, Accuracy: 61.00%\n","Epoch [2/10], Step [1100/1298], Loss: 1.0046, Accuracy: 52.00%\n","Epoch [2/10], Step [1200/1298], Loss: 0.8959, Accuracy: 64.00%\n","Epoch [3/10], Step [100/1298], Loss: 1.0784, Accuracy: 52.00%\n","Epoch [3/10], Step [200/1298], Loss: 0.9236, Accuracy: 57.00%\n","Epoch [3/10], Step [300/1298], Loss: 1.0713, Accuracy: 54.00%\n","Epoch [3/10], Step [400/1298], Loss: 1.0206, Accuracy: 59.00%\n","Epoch [3/10], Step [500/1298], Loss: 1.0699, Accuracy: 62.00%\n","Epoch [3/10], Step [600/1298], Loss: 1.1047, Accuracy: 47.00%\n","Epoch [3/10], Step [700/1298], Loss: 1.0536, Accuracy: 52.00%\n","Epoch [3/10], Step [800/1298], Loss: 1.0111, Accuracy: 57.00%\n","Epoch [3/10], Step [900/1298], Loss: 0.8222, Accuracy: 68.00%\n","Epoch [3/10], Step [1000/1298], Loss: 0.8776, Accuracy: 63.00%\n","Epoch [3/10], Step [1100/1298], Loss: 1.0237, Accuracy: 54.00%\n","Epoch [3/10], Step [1200/1298], Loss: 0.9016, Accuracy: 69.00%\n","Epoch [4/10], Step [100/1298], Loss: 0.9975, Accuracy: 53.00%\n","Epoch [4/10], Step [200/1298], Loss: 0.8787, Accuracy: 66.00%\n","Epoch [4/10], Step [300/1298], Loss: 1.0398, Accuracy: 60.00%\n","Epoch [4/10], Step [400/1298], Loss: 0.9168, Accuracy: 61.00%\n","Epoch [4/10], Step [500/1298], Loss: 1.0403, Accuracy: 55.00%\n","Epoch [4/10], Step [600/1298], Loss: 1.0049, Accuracy: 57.00%\n","Epoch [4/10], Step [700/1298], Loss: 1.0667, Accuracy: 50.00%\n","Epoch [4/10], Step [800/1298], Loss: 0.9727, Accuracy: 62.00%\n","Epoch [4/10], Step [900/1298], Loss: 0.7979, Accuracy: 69.00%\n","Epoch [4/10], Step [1000/1298], Loss: 0.9113, Accuracy: 59.00%\n","Epoch [4/10], Step [1100/1298], Loss: 0.9494, Accuracy: 52.00%\n","Epoch [4/10], Step [1200/1298], Loss: 0.8625, Accuracy: 65.00%\n","Epoch [5/10], Step [100/1298], Loss: 1.0508, Accuracy: 58.00%\n","Epoch [5/10], Step [200/1298], Loss: 0.9024, Accuracy: 54.00%\n","Epoch [5/10], Step [300/1298], Loss: 1.0848, Accuracy: 58.00%\n","Epoch [5/10], Step [400/1298], Loss: 0.9621, Accuracy: 60.00%\n","Epoch [5/10], Step [500/1298], Loss: 1.0520, Accuracy: 56.00%\n","Epoch [5/10], Step [600/1298], Loss: 1.0646, Accuracy: 60.00%\n","Epoch [5/10], Step [700/1298], Loss: 1.0377, Accuracy: 56.00%\n","Epoch [5/10], Step [800/1298], Loss: 0.8914, Accuracy: 67.00%\n","Epoch [5/10], Step [900/1298], Loss: 0.8007, Accuracy: 70.00%\n","Epoch [5/10], Step [1000/1298], Loss: 0.8908, Accuracy: 63.00%\n","Epoch [5/10], Step [1100/1298], Loss: 0.9595, Accuracy: 61.00%\n","Epoch [5/10], Step [1200/1298], Loss: 0.9497, Accuracy: 65.00%\n","Epoch [6/10], Step [100/1298], Loss: 0.9351, Accuracy: 60.00%\n","Epoch [6/10], Step [200/1298], Loss: 0.8554, Accuracy: 67.00%\n","Epoch [6/10], Step [300/1298], Loss: 1.0506, Accuracy: 56.00%\n","Epoch [6/10], Step [400/1298], Loss: 1.0081, Accuracy: 64.00%\n","Epoch [6/10], Step [500/1298], Loss: 0.9620, Accuracy: 56.00%\n","Epoch [6/10], Step [600/1298], Loss: 1.0117, Accuracy: 54.00%\n","Epoch [6/10], Step [700/1298], Loss: 1.0317, Accuracy: 56.00%\n","Epoch [6/10], Step [800/1298], Loss: 0.8951, Accuracy: 64.00%\n","Epoch [6/10], Step [900/1298], Loss: 0.8037, Accuracy: 67.00%\n","Epoch [6/10], Step [1000/1298], Loss: 0.8647, Accuracy: 66.00%\n","Epoch [6/10], Step [1100/1298], Loss: 0.9241, Accuracy: 61.00%\n","Epoch [6/10], Step [1200/1298], Loss: 0.8565, Accuracy: 67.00%\n","Epoch [7/10], Step [100/1298], Loss: 0.9420, Accuracy: 60.00%\n","Epoch [7/10], Step [200/1298], Loss: 0.8756, Accuracy: 67.00%\n","Epoch [7/10], Step [300/1298], Loss: 1.1264, Accuracy: 57.00%\n","Epoch [7/10], Step [400/1298], Loss: 0.9122, Accuracy: 64.00%\n","Epoch [7/10], Step [500/1298], Loss: 0.9884, Accuracy: 56.00%\n","Epoch [7/10], Step [600/1298], Loss: 1.0262, Accuracy: 60.00%\n","Epoch [7/10], Step [700/1298], Loss: 1.1231, Accuracy: 52.00%\n","Epoch [7/10], Step [800/1298], Loss: 0.9300, Accuracy: 67.00%\n","Epoch [7/10], Step [900/1298], Loss: 0.8261, Accuracy: 67.00%\n","Epoch [7/10], Step [1000/1298], Loss: 0.9003, Accuracy: 61.00%\n","Epoch [7/10], Step [1100/1298], Loss: 0.9648, Accuracy: 57.00%\n","Epoch [7/10], Step [1200/1298], Loss: 0.7858, Accuracy: 74.00%\n","Epoch [8/10], Step [100/1298], Loss: 0.9520, Accuracy: 62.00%\n","Epoch [8/10], Step [200/1298], Loss: 0.9156, Accuracy: 64.00%\n","Epoch [8/10], Step [300/1298], Loss: 0.9768, Accuracy: 58.00%\n","Epoch [8/10], Step [400/1298], Loss: 0.9646, Accuracy: 57.00%\n","Epoch [8/10], Step [500/1298], Loss: 0.9881, Accuracy: 60.00%\n","Epoch [8/10], Step [600/1298], Loss: 1.0569, Accuracy: 51.00%\n","Epoch [8/10], Step [700/1298], Loss: 1.0237, Accuracy: 58.00%\n","Epoch [8/10], Step [800/1298], Loss: 0.9880, Accuracy: 58.00%\n","Epoch [8/10], Step [900/1298], Loss: 0.7727, Accuracy: 72.00%\n","Epoch [8/10], Step [1000/1298], Loss: 0.8789, Accuracy: 66.00%\n","Epoch [8/10], Step [1100/1298], Loss: 0.9670, Accuracy: 59.00%\n","Epoch [8/10], Step [1200/1298], Loss: 0.7807, Accuracy: 75.00%\n","Epoch [9/10], Step [100/1298], Loss: 1.0472, Accuracy: 51.00%\n","Epoch [9/10], Step [200/1298], Loss: 0.8930, Accuracy: 64.00%\n","Epoch [9/10], Step [300/1298], Loss: 1.0458, Accuracy: 51.00%\n","Epoch [9/10], Step [400/1298], Loss: 1.0414, Accuracy: 62.00%\n","Epoch [9/10], Step [500/1298], Loss: 0.9135, Accuracy: 57.00%\n","Epoch [9/10], Step [600/1298], Loss: 1.0413, Accuracy: 53.00%\n","Epoch [9/10], Step [700/1298], Loss: 1.0731, Accuracy: 51.00%\n","Epoch [9/10], Step [800/1298], Loss: 0.8740, Accuracy: 71.00%\n","Epoch [9/10], Step [900/1298], Loss: 0.7864, Accuracy: 67.00%\n","Epoch [9/10], Step [1000/1298], Loss: 0.9052, Accuracy: 63.00%\n","Epoch [9/10], Step [1100/1298], Loss: 1.0301, Accuracy: 58.00%\n","Epoch [9/10], Step [1200/1298], Loss: 0.7835, Accuracy: 66.00%\n","Epoch [10/10], Step [100/1298], Loss: 1.0401, Accuracy: 56.00%\n","Epoch [10/10], Step [200/1298], Loss: 0.8762, Accuracy: 67.00%\n","Epoch [10/10], Step [300/1298], Loss: 1.0003, Accuracy: 61.00%\n","Epoch [10/10], Step [400/1298], Loss: 0.9579, Accuracy: 65.00%\n","Epoch [10/10], Step [500/1298], Loss: 0.9498, Accuracy: 60.00%\n","Epoch [10/10], Step [600/1298], Loss: 0.9659, Accuracy: 58.00%\n","Epoch [10/10], Step [700/1298], Loss: 1.1296, Accuracy: 56.00%\n","Epoch [10/10], Step [800/1298], Loss: 0.8249, Accuracy: 68.00%\n","Epoch [10/10], Step [900/1298], Loss: 0.7364, Accuracy: 71.00%\n","Epoch [10/10], Step [1000/1298], Loss: 0.7321, Accuracy: 63.00%\n","Epoch [10/10], Step [1100/1298], Loss: 0.9033, Accuracy: 66.00%\n","Epoch [10/10], Step [1200/1298], Loss: 0.7927, Accuracy: 74.00%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Se7j3eO8AvMx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"118c51b8-47c4-46c4-963a-dc91f4c017da","executionInfo":{"status":"ok","timestamp":1585480792894,"user_tz":-330,"elapsed":3933,"user":{"displayName":"Mohamed Nazeem Ayoob","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GinvibcgKeZsjehPDC2jk5BI24GJbHkt4ePsM29Dw=s64","userId":"15887030072782416774"}}},"source":["# Test the model\n","modelConv.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        if len(images) != 100:\n","          continue\n","        labels = labels.to(device)\n","        outputs = modelConv(images.reshape(100,1,7,12).to(device))\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on test images: {} %'.format((correct / total) * 100))\n","\n","# Save the model and plot\n","torch.save(modelConv.state_dict(), '/' + 'conv_net_model.ckpt')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Test Accuracy of the model on the 10000 test images: 63.13669064748202 %\n"],"name":"stdout"}]}]}