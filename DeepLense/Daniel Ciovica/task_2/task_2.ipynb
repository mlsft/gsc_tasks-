{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\n"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "n_anomalies= 5  # number of top anomalies\n",
    "MODEL_TRAINED = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable n_anomalies can be changed to show any number of anomalies, according to the user's preference. \n",
    "MODEL_TRAINED can be changed to True if the model has already been trained, for faster re-runs. \n",
    "The device variable will be \"cuda\" if the cuda package was installed with pytorch, or \"cpu\", otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(path, batch_size=batch_size, shuffle=False):\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((148,148)),  # Resize the images to (148,148)\n",
    "                transforms.ToTensor(),  # convert from numpy images to torch images\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize the images in the [-1,1] range\n",
    "    ]) \n",
    "    image_set = datasets.ImageFolder(root=path, transform=transform)\n",
    "    image_set_loader = torch.utils.data.DataLoader(image_set, batch_size=batch_size, shuffle=shuffle, num_workers=4)\n",
    "\n",
    "    return image_set_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I resized the images to (148,148), normalized them in the [-1, 1] range and converted them from numpy images to torch images. All these modifications were implemented in the Compose function. Moreover, I didn't shuffle the images to keep the images' index for plotting reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATASET = \"./lenses\"\n",
    "image_set_loader = image_loader(PATH_DATASET)  # create the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PATH_DATASET variable should be changed with the current location of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(  # encoder - used for learning purposes\n",
    "            nn.Conv2d(3, 62, 3, stride=3, padding=1),  # 1 st hidden layer\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # 2nd hidden layer\n",
    "            nn.Conv2d(62, 31, 3, stride=2, padding=1),  # 3rd hidden layer\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # 4th hidden layer\n",
    "        )\n",
    "        self.decoder = nn.Sequential(  # decoder - used for reconstructing the image\n",
    "            nn.ConvTranspose2d(31, 62, 3, stride=2),  # 5th hidden layer\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(62, 31, 5, stride=3, padding=1),  # 6th hidden layer\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(31, 3, 2, stride=2, padding=1),  # 7th hidden layer\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the anomaly detection problem in the given dataset, I used a convolutional neural network with an autoencoder. The model has two stages: the first one in which the images are encoded for learning purposes, and the second one in which the images are decoded for reconstruction. The main idea of an autoencoder in the anomaly detection problem is to reconstruct the initial image and calculate the corresponding error. The images with the highest error will be considered anomalies. In order to detect anomalies one can either set a threshold for the error function, thus everything above that threshold is considered anomaly, or pick the n biggest errors and categorise them as anomalies. I have used the latter approch. \n",
    "\n",
    "For the encoder, a sequential container was used with Conv2d as the convolutional layers, ReLU as the activation functions and  MaxPool2d as dimensionality reduction layers. \n",
    "\n",
    "For the decoder, the above statements were reverted, using the ConvTranspose2d as the \"deconvolution\" function, ReLU as activation function and, in the end, the Tanh function was applied at the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TRAINED == False:\n",
    "    model = AutoEncoder().to(device, non_blocking=True)  # create the model\n",
    "    error_function = nn.MSELoss()  # mean squared error function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay=1e-5)  # adam optimizer\n",
    "    for epoch in range(num_epochs):  \n",
    "        for data in image_set_loader:\n",
    "            img, labels = data\n",
    "            img = Variable(img).to(device, non_blocking=True)\n",
    "            # forward propagation\n",
    "            output = model(img)\n",
    "            loss = error_function(output, img)\n",
    "            # backward propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # log\n",
    "        print('epoch [{}/{}], loss:{:.4f}'\n",
    "            .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training part, the considered loss function for the created model was mean squared error from the torch.nn library. The chosen optimizer was Adam with standard model parameters, learning rate of 1e-3 and weight decay of 1e-5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TRAINED == False:  # save the trained model\n",
    "    PATH = './cnn_model.pth'\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TRAINED == True:  # load the trained model\n",
    "    PATH = './cnn_model.pth'\n",
    "    model=AutoEncoder().to(device, non_blocking=True)\n",
    "    model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[77, 144, 146, 18, 6]\n"
    }
   ],
   "source": [
    "list_max_error = list()\n",
    "for batch_id, data in enumerate(image_set_loader):\n",
    "  X, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)  # get the image\n",
    "  Y = model(X)  # reconstruct the image\n",
    "  N = len(X)\n",
    "  max_error = 0.0\n",
    "  for i in range(N):\n",
    "    curr_error = torch.sum((X[i]-Y[i])*(X[i]-Y[i]))  # calculate the error\n",
    "    if curr_error.item() > max_error:\n",
    "      max_error = curr_error.item()\n",
    "  list_max_error.append(max_error)  # create a list with all the errors for each image\n",
    "list_max_error = sorted(range(len(list_max_error)), key=lambda i: list_max_error[i])[-n_anomalies:]  # sort the images in descending order and pick the first n_anomalies\n",
    "\n",
    "print(list_max_error)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above block the reconstruction takes part. Therefore, the error between the input image and the reconstructed one is calculated, the biggest n values being stored in list_max_error. \n",
    "\n",
    "In the below section the images considered anomalies are plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nOne of the highest reconstruction error has the index  6\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nOne of the highest reconstruction error has the index  18\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nOne of the highest reconstruction error has the index  77\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nOne of the highest reconstruction error has the index  144\nOne of the highest reconstruction error has the index  146\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 10 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"194.227774pt\" version=\"1.1\" viewBox=\"0 0 352.7 194.227774\" width=\"352.7pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 194.227774 \nL 352.7 194.227774 \nL 352.7 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 64.924138 \nL 68.424138 64.924138 \nL 68.424138 7.2 \nL 10.7 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p40f76805a8)\">\n    <image height=\"58\" id=\"imagef4c3c1760d\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAI9JREFUaIHt2cERwyAMBdHvtEAVlERRtAetcIursJJZ7ysAsTfN6EryzQt8fv2BKobSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSlIb23jPnzBijcmyS4tC9d9ZaOedUjk2SXPGaxmIojaE0htIYSvN3oa21R951BaQxlMZQGkNpDKUxlMZQGkNpXhN6AxtjDsTDaA6lAAAAAElFTkSuQmCC\" y=\"-6.924138\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 64.924138 \nL 10.7 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 68.424138 64.924138 \nL 68.424138 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 64.924138 \nL 68.424138 64.924138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 7.2 \nL 68.424138 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 10.7 183.527774 \nL 68.424138 183.527774 \nL 68.424138 125.803636 \nL 10.7 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd23091f21e)\">\n    <image height=\"58\" id=\"image04f61e2740\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAIBJREFUaIHt2aERgEAQBMF9ssNAJrwhWDwCh0JhIIujaphOYG/0tSRPfmD4+oAqhtIYSmMojaE0htIYSmMojaE0htIYSmMojaE0htKUh/ZlzTyN1bP1ofux5Trv6tm0+E1jMZTGUBpDaQylMZTGUBpDaQylMZTGUBpDaQylMZTmBQTMCiYIhCFxAAAAAElFTkSuQmCC\" y=\"-125.527774\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 10.7 183.527774 \nL 10.7 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 68.424138 183.527774 \nL 68.424138 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 10.7 183.527774 \nL 68.424138 183.527774 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 10.7 125.803636 \nL 68.424138 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 79.968966 64.924138 \nL 137.693103 64.924138 \nL 137.693103 7.2 \nL 79.968966 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p2bf25e1007)\">\n    <image height=\"58\" id=\"imagec6bbcc3324\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"79.968966\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAJNJREFUaIHt2bENxCAQBdH1ZRRAQlvURke0QR9kdhPcWhrPK4DPZEhcEXHHB/zevkAWQ2kMpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlAaQ/+tlJK6lx7ae48xRrTWUnfTQ/feMeeMtVbq7hX+prEYSmMojaE0R0NrrSePO8oHA42hNIbSGEpjKI2hNIbSGEpjKM1nQh/Xkw6a4nZu2QAAAABJRU5ErkJggg==\" y=\"-6.924138\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 79.968966 64.924138 \nL 79.968966 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 137.693103 64.924138 \nL 137.693103 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 79.968966 64.924138 \nL 137.693103 64.924138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 79.968966 7.2 \nL 137.693103 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 79.968966 183.527774 \nL 137.693103 183.527774 \nL 137.693103 125.803636 \nL 79.968966 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p9f3800b274)\">\n    <image height=\"58\" id=\"image086cb63adf\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"79.968966\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAKBJREFUaIHt2TEKwkAYBeEXEZWAtkJuJTYeRXPTYO8BDBZWeon1FybzHWDfTrewXZJPFmD17wtUMZTGUBpDaQylMZTGUBpDaQylMZTGUBpDaQylMfSX+l1fvlkeej5dchuv2W8Ppbvr0rUk8+uR+7TJ/H6W7nbxN43FUBpDaQylaRo6HIeWxzXlg4HGUBpDaQylMZTGUBpDaQylMZRmMaFfp1sNx9So10QAAAAASUVORK5CYII=\" y=\"-125.527774\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 79.968966 183.527774 \nL 79.968966 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 137.693103 183.527774 \nL 137.693103 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 79.968966 183.527774 \nL 137.693103 183.527774 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 79.968966 125.803636 \nL 137.693103 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 149.237931 64.924138 \nL 206.962069 64.924138 \nL 206.962069 7.2 \nL 149.237931 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p2732d00165)\">\n    <image height=\"58\" id=\"imagefebb37ae20\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"149.237931\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAKlJREFUaIHt17ENwyAYROFzKJAoYRm2oGabDOk53FOgZAYrgkjn9w3wH6/kkPTRA7z+/YBdCHVDqBtC3RDqhlA3hLoh9FelFMUYV52/bVlozlljjFXnbzvEx9sLoW4IdUOoG0LdELpKa029992z+0NTSqq17p5VkPTeOXiep+acCiHouq5tu/xe3BDqhlA3hLoh1A2hbgh1Q6gbQt0Q6oZQN4S6IdTNY0K/QDMSY0hbUGAAAAAASUVORK5CYII=\" y=\"-6.924138\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 149.237931 64.924138 \nL 149.237931 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 206.962069 64.924138 \nL 206.962069 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 149.237931 64.924138 \nL 206.962069 64.924138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 149.237931 7.2 \nL 206.962069 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 149.237931 183.527774 \nL 206.962069 183.527774 \nL 206.962069 125.803636 \nL 149.237931 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p2e3ffb05f3)\">\n    <image height=\"58\" id=\"imagea3237d56f1\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"149.237931\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAK9JREFUaIHt1zEKwkAQQNFJiqggK1ZCeiuLIGIt5BiCeP/WRhFhPUOQXeHnP9h2Zn85TUTkmIH23x+oxVAaQ2kMpTGUxlAaQ2kM/dWu72OVlqXGT1Ys9P3J8Xy8So2frAkPbxZDaQylMZTGUBpDSxkvY9zu19pr64em1MVh2Md6u6m9OnLtdzqec9cuqu70eqExlMZQGkNpDKUxlMZQGkNpDKUxlMZQGkNpDKWZTegX4U1GwT//IXYAAAAASUVORK5CYII=\" y=\"-125.527774\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 149.237931 183.527774 \nL 149.237931 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 206.962069 183.527774 \nL 206.962069 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 149.237931 183.527774 \nL 206.962069 183.527774 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 149.237931 125.803636 \nL 206.962069 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 218.506897 64.924138 \nL 276.231034 64.924138 \nL 276.231034 7.2 \nL 218.506897 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p95e4b112ae)\">\n    <image height=\"58\" id=\"imagefb5710eb34\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"218.506897\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAHdJREFUaIHt2cEJwEAIBdG/26jYfwtC0oWBybwGdMCbJ8mTH7hfL7DFUBpDaQylMZTGUBpDaQylMZTGUBpDaQylMZTGUJr10O5OVW2P3Q+dmdy7f0gnftNYDKUxlMZQGkNpDKUxlMZQGkNpDKUxlMZQGkNpDKV5AbXjB6sYlhD0AAAAAElFTkSuQmCC\" y=\"-6.924138\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 218.506897 64.924138 \nL 218.506897 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 276.231034 64.924138 \nL 276.231034 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 218.506897 64.924138 \nL 276.231034 64.924138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 218.506897 7.2 \nL 276.231034 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 218.506897 183.527774 \nL 276.231034 183.527774 \nL 276.231034 125.803636 \nL 218.506897 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pe722f1d56a)\">\n    <image height=\"58\" id=\"image8c260306a3\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"218.506897\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAIJJREFUaIHt2bEJgFAQBNH9aiYYWY+ZYAPy++/CRLSLE8Z5DexNfC3Jkx8Yvj6giqE0htIYSmMojaE0htIYSmMojaE0htIYSmMojaE05aH97Dn2rXo2U/XgvIy5r7V6Ni1+01gMpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlAaQ2kMpXkBVc4GZqm6LwkAAAAASUVORK5CYII=\" y=\"-125.527774\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 218.506897 183.527774 \nL 218.506897 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 276.231034 183.527774 \nL 276.231034 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 218.506897 183.527774 \nL 276.231034 183.527774 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 218.506897 125.803636 \nL 276.231034 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g id=\"patch_42\">\n    <path d=\"M 287.775862 64.924138 \nL 345.5 64.924138 \nL 345.5 7.2 \nL 287.775862 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd41ae1e65d)\">\n    <image height=\"58\" id=\"image4b12b6876c\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"287.775862\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAIpJREFUaIHt2EERwCAMBdHQCyLQgCYO+LeQVkXDzLLPwGdvGVpEvHGB5/QDqhhKYyiNoTSG0hhKYyiNoTSG/m2MUbrXwh8GFkNpDKUxlMZQmvLQvXestapn60MzMzKzetZbF8dQGkNpDKU5GjrnjN57yZYHA42hNIbSGEpjKI2hNIbSGEpjKM01oR9DzA2Ui3o3mwAAAABJRU5ErkJggg==\" y=\"-6.924138\"/>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 287.775862 64.924138 \nL 287.775862 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 345.5 64.924138 \nL 345.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 287.775862 64.924138 \nL 345.5 64.924138 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 287.775862 7.2 \nL 345.5 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_10\">\n   <g id=\"patch_47\">\n    <path d=\"M 287.775862 183.527774 \nL 345.5 183.527774 \nL 345.5 125.803636 \nL 287.775862 125.803636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p37ece3a65d)\">\n    <image height=\"58\" id=\"image63b320c418\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"287.775862\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAAAH9JREFUaIHt2cEJgDAQBdEfiUf78hhQSP81WEDsYoVxXgN/57wtycoPbF8fUMVQGkNpDKUxlMZQGkNpDKUxlMZQGkNpDKUxlMZQmvLQec9c46yeTa8ebPuTvo7q2bT4TWMxlMZQGkNpDKUxlMZQGkNpDKUxlMZQGkNpDKUxlOYFwVcGaS1m7PkAAAAASUVORK5CYII=\" y=\"-125.527774\"/>\n   </g>\n   <g id=\"patch_48\">\n    <path d=\"M 287.775862 183.527774 \nL 287.775862 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path d=\"M 345.5 183.527774 \nL 345.5 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path d=\"M 287.775862 183.527774 \nL 345.5 183.527774 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path d=\"M 287.775862 125.803636 \nL 345.5 125.803636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p40f76805a8\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"10.7\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"pd23091f21e\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"10.7\" y=\"125.803636\"/>\n  </clipPath>\n  <clipPath id=\"p2bf25e1007\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"79.968966\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p9f3800b274\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"79.968966\" y=\"125.803636\"/>\n  </clipPath>\n  <clipPath id=\"p2732d00165\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"149.237931\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p2e3ffb05f3\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"149.237931\" y=\"125.803636\"/>\n  </clipPath>\n  <clipPath id=\"p95e4b112ae\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"218.506897\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"pe722f1d56a\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"218.506897\" y=\"125.803636\"/>\n  </clipPath>\n  <clipPath id=\"pd41ae1e65d\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"287.775862\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p37ece3a65d\">\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"287.775862\" y=\"125.803636\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "i = 1\n",
    "for batch_id, data in enumerate(image_set_loader):\n",
    "  X, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True) # get the image \n",
    "  Y = model(X)  # reconstruct the image\n",
    "  if batch_id in list_max_error:\n",
    "    print(\"One of the highest reconstruction error has the index \", batch_id)\n",
    "    ax = plt.subplot(2,n_anomalies,i)  # plot the first n_anomalies\n",
    "    plt.imshow(X[0].cpu().permute(2,1,0))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2,n_anomalies,i + n_anomalies)\n",
    "    plt.imshow(Y[0].cpu().detach().numpy().transpose(2,1,0))\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    i += 1 \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}