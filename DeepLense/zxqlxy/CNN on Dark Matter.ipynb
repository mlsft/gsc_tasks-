{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose(\n",
    "    [\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "class LensesDataset(list):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files_sub = [f for f in listdir(root_dir + \"/sub\") if isfile(join(root_dir + \"/sub\", f))]\n",
    "        self.files_nosub = [f for f in listdir(root_dir + \"/no_sub\") if isfile(join(root_dir + \"/no_sub\", f))]\n",
    "        self.data = []\n",
    "        \n",
    "        assert(len(self.files_sub) == 5000)\n",
    "        assert(len(self.files_nosub) == 5000)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= 5000:\n",
    "            img_name = os.path.join(self.root_dir + \"/sub\",\n",
    "                                self.files_sub[idx-5000])\n",
    "            image = io.imread(img_name)\n",
    "            image = np.array(image)\n",
    "            sample = {'image': image, 'label':1}\n",
    "\n",
    "        else:\n",
    "            img_name = os.path.join(self.root_dir + \"/no_sub\",\n",
    "                                self.files_nosub[idx])\n",
    "            image = io.imread(img_name)\n",
    "            image = np.array(image)\n",
    "            sample = {'image': image, 'label':0}\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def construct(self):\n",
    "        for i in range(10000):\n",
    "            self.data.append(self.__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading dataset\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "lenses_dataset = LensesDataset(root_dir='lenses')\n",
    "\n",
    "lenses_dataset.construct()\n",
    "print(\"finished loading dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9697 (150, 150) 1\n",
      "6356 (150, 150) 1\n",
      "7550 (150, 150) 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inspect some of the data\n",
    "import random as rand\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "count = 0\n",
    "for i in range(len(lenses_dataset.data)):\n",
    "    count +=1\n",
    "    num = rand.randint(0, len(lenses_dataset.data))\n",
    "    sample = lenses_dataset.data[num]\n",
    "\n",
    "    print(num, sample['image'].shape, sample['label'])\n",
    "\n",
    "    ax = plt.subplot(1, 4, count)\n",
    "#     plt.tight_layout()\n",
    "    plt.imshow(sample['image'], cmap = \"gray\")\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "\n",
    "    if count == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net,self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, kernel_size=5,padding=2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "#         self.conv3 = nn.Conv2d(16,120,kernel_size=5)\n",
    "#         self.mp = nn.MaxPool2d(2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc1 = nn.Linear(120,84)\n",
    "#         self.fc2 = nn.Linear(84,10)\n",
    "#         self.logsoftmax = nn.LogSoftmax()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         in_size = x.size(0)\n",
    "#         out = self.relu(self.mp(self.conv1(x)))\n",
    "#         out = self.relu(self.mp(self.conv2(out)))\n",
    "#         out = self.relu(self.conv3(out))\n",
    "#         out = out.view(in_size, -1)\n",
    "#         out = self.relu(self.fc1(out))\n",
    "#         out = self.fc2(out)\n",
    "#         return self.logsoftmax(out)\n",
    "    \n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(3, 3)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 1)\n",
    "#         self.fc1 = nn.Linear(32, 16)\n",
    "#         self.fc2 = nn.Linear(16, 8)\n",
    "#         self.fc3 = nn.Linear(8, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 32)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 15 * 15, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, 32 * 15 * 15)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         print(x.shape)\n",
    "        x = self.fc3(x)\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=7200, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 146, 146]             156\n",
      "         MaxPool2d-2            [-1, 6, 73, 73]               0\n",
      "            Conv2d-3           [-1, 16, 69, 69]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 34, 34]               0\n",
      "            Conv2d-5           [-1, 32, 30, 30]          12,832\n",
      "         MaxPool2d-6           [-1, 32, 15, 15]               0\n",
      "            Linear-7                  [-1, 120]         864,120\n",
      "            Linear-8                   [-1, 84]          10,164\n",
      "            Linear-9                    [-1, 2]             170\n",
      "================================================================\n",
      "Total params: 889,858\n",
      "Trainable params: 889,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 2.22\n",
      "Params size (MB): 3.39\n",
      "Estimated Total Size (MB): 5.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect the summary\n",
    "from torchsummary import summary\n",
    "print(net)\n",
    "summary(net, (1, 150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 150, 150)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = []\n",
    "for i in range(10000):\n",
    "    scaler.fit_transform(lenses_dataset.data[i][\"image\"])\n",
    "    X.append(lenses_dataset.data[i][\"image\"])\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "Y = [0 for i in range(5000)]\n",
    "Y.extend([1 for i in range(5000)])\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((8000, 150, 150), (8000,)), ((2000, 150, 150), (2000,)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(X, Y, test_size = 0.2)\n",
    "(train_x.shape, train_y.shape), (val_x.shape, val_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8000, 1, 150, 150]), torch.Size([8000]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting training images into torch format\n",
    "train_x = train_x.reshape(8000, 1, 150, 150)\n",
    "train_x  = torch.from_numpy(train_x).float()\n",
    "\n",
    "train_y = train_y.astype(int);\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "# shape of training data\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2000, 1, 150, 150]), torch.Size([2000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting validation images into torch format\n",
    "val_x = val_x.reshape(2000, 1, 150, 150)\n",
    "val_x  = torch.from_numpy(val_x).float()\n",
    "\n",
    "val_y = val_y.astype(int);\n",
    "val_y = torch.from_numpy(val_y)\n",
    "\n",
    "# shape of validation data\n",
    "val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x, train_y)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,      # torch TensorDataset format\n",
    "    batch_size=32,      # mini batch size\n",
    "    shuffle=True,               \n",
    "    num_workers=2,              \n",
    ")\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(val_x, val_y)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,      # torch TensorDataset format\n",
    "    batch_size=100,      # mini batch size\n",
    "    shuffle=True,              \n",
    "    num_workers=2,             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data# [\"image\"],  data[\"label\"]\n",
    "\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = net(inputs)\n",
    "\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 20 == 19:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.10f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 20))\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './cifar_net.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for more training\n",
    "net.load_state_dict(torch.load('./model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 \tTraining Loss: 0.005594 \tValidation Loss: 0.001468\n",
      "\t\tTest Accuracy:   93% (1875/2000)\n",
      "\t\tValidation loss decreased (inf --> 0.001468).  Saving model ...\n",
      "Epoch: 2/5 \tTraining Loss: 0.005876 \tValidation Loss: 0.001389\n",
      "\t\tTest Accuracy:   93% (3750/4000)\n",
      "\t\tValidation loss decreased (0.001468 --> 0.001389).  Saving model ...\n",
      "Epoch: 3/5 \tTraining Loss: 0.005075 \tValidation Loss: 0.001524\n",
      "\t\tTest Accuracy:   93% (5612/6000)\n",
      "Epoch: 4/5 \tTraining Loss: 0.004894 \tValidation Loss: 0.001354\n",
      "\t\tTest Accuracy:   93% (7495/8000)\n",
      "\t\tValidation loss decreased (0.001389 --> 0.001354).  Saving model ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 81696) is killed by signal: Unknown signal: 0. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4aabb3a7c59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prep model for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Move input and label tensors to the default device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         data, target = data.to(device), target.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 81696) is killed by signal: Unknown signal: 0. "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "# compare overfited\n",
    "train_loss_data,valid_loss_data = [],[]\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    net.train() # prep model for training\n",
    "    for data, target in trainloader:\n",
    "        # Move input and label tensors to the default device\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = net(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item() #*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    net.eval() # prep model for evaluation\n",
    "    for data, target in valloader:\n",
    "        # Move input and label tensors to the default device\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = net(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item() #*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(100):\n",
    "          label = target.data[i]\n",
    "          class_correct[label] += correct[i].item()\n",
    "          class_total[label] += 1\n",
    "        \n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(valloader.dataset)\n",
    "    \n",
    "    # calculate train loss and running loss\n",
    "    train_loss_data.append(train_loss)\n",
    "    valid_loss_data.append(valid_loss)\n",
    "    \n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        n_epochs,\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    print('\\t\\tTest Accuracy: %4d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('\\t\\tValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(net.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for overfitting\n",
    "# plt.plot(train_loss_data, label = \"taining loss\")\n",
    "# plt.plot(valid_loss_data, label = \"validation loss\")\n",
    "# plt.legend(frameon = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
