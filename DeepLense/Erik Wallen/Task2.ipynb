{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Erik Wallin\n",
    "Task 2\n",
    "\n",
    "Using autoencoders, we get an unsupervised network that learns a representation of the data in \n",
    "the small bottle-neck layer, the latent space. The latent space gives us a dimensionality\n",
    "reduction, which shows that it has learned some representation of the data.\n",
    "\n",
    "For anomaly detection we need an unsupervised training method since we can't easily label wether an\n",
    "experimental image has a DM-substructure or not. If we could, that would somewhat defeat the purpose. \n",
    "\n",
    "I will train the autoencoder on both images with dark matter-substructure and without, so that one can use\n",
    "anomaly detection to find experimental images that does not fit either these models. \n",
    "\n",
    "One could also train two such autoencoders on only one dataset each, to compare how they \n",
    "reconstruct the experimental data. \n",
    "\n",
    "I did not see this as a task about classification, i.e. whether or not a certain image has some\n",
    "dark matter substructure or not. For that some supervised classification method should be used. \n",
    "\n",
    "This proof of concept uses a purely linear network, with the AdamW optimizer, a SGD-like variant with weight-decay\n",
    "which often shows good results for quick learning. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "data_path =\"../lenses\"\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(\n",
    "    root=data_path,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Grayscale(1),\n",
    "        torchvision.transforms.Resize((50,50)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "\n",
    "train,_ = torch.utils.data.random_split(dataset, [8000,2000])\n",
    "\n",
    "\n",
    "#The data is naturally in a suitable range from 0 to 1 after being grayscaled. Also,\n",
    "#the mean value of the data is somewhere around 'black', so it doesn't need to be normalized much.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          )\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(train,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "One should figure out some more intelligent loss function, since most of the image will be \n",
    "black space quite naturally. One could give it more weight to areas with actual activity. \n",
    "\"\"\"\n",
    "lossfunc = nn.MSELoss()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implements a 50*50 - 1000 - 1000 - 500 - 1000- 1000 - 50*50 linear network\n",
    "Thats a 5-time dimensionality reduction to the latent space. \n",
    "With time one could do some nice hyper-parameter scans to find an optimal network structure. \n",
    "\"\"\"\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.n1 = nn.Linear(50*50,1000)\n",
    "        self.n2 = nn.Linear(1000,1000)\n",
    "        self.n3 = nn.Linear(1000,500)\n",
    "        self.n4 = nn.Linear(500,1000)\n",
    "        self.n5 = nn.Linear(1000,1000)\n",
    "        self.n6 = nn.Linear(1000,50*50)\n",
    "        \n",
    "        self.func = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.n3(self.func(self.n2(self.func(self.n1(x)))))\n",
    "        return self.n6(self.func(self.n5(self.func(self.n4(self.func(encoded))))))\n",
    "\n",
    "model = Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07815547697246075\n",
      "0.05660406120121479\n",
      "0.04258782185614109\n",
      "0.03237452702596784\n",
      "0.024387747403234242\n",
      "0.018783607957884668\n",
      "0.014794549737125635\n",
      "0.011167375966906548\n",
      "0.008862074352800847\n",
      "0.006958424090407788\n",
      "0.005597779592499137\n",
      "0.004511185938026756\n",
      "0.003842435907572508\n",
      "0.0032210370036773383\n",
      "0.002696759554091841\n",
      "0.0022303017100784926\n",
      "0.00209053571568802\n",
      "0.0018259528710041196\n",
      "0.001771223340765573\n",
      "0.0016263902379432694\n",
      "0.001558453066390939\n",
      "0.0016015682026045397\n",
      "0.001496130115701817\n",
      "0.0015366951160831377\n",
      "0.0014643413631711154\n",
      "0.0015739518363261595\n",
      "0.001471261830884032\n",
      "0.0013547899192781188\n",
      "0.0013891249499283732\n",
      "0.0015452419803477823\n",
      "0.0013551101263146847\n",
      "0.0013223755138460547\n",
      "0.001534765813848935\n",
      "0.0014358618465485052\n",
      "0.0014101557707181201\n",
      "0.0014366814238019289\n",
      "0.0015892190803424456\n",
      "0.0014738166576717048\n",
      "0.0014629695599433035\n",
      "0.0015501493751071394\n",
      "0.0013248483563074842\n",
      "0.0012914605240803212\n",
      "0.0011914023023564368\n",
      "0.0015264532389119268\n",
      "0.0016372486966429278\n",
      "0.0015044055751059205\n",
      "0.0015164335549343378\n",
      "0.0013593171242973767\n",
      "0.001444684480666183\n",
      "0.0013707532518310473\n",
      "0.0014593584841350094\n",
      "0.0016385826148325577\n",
      "0.0014548195176757872\n",
      "0.0014993906137533487\n",
      "0.0014608699473319575\n",
      "0.0016739120802958497\n",
      "0.0014344844422885218\n",
      "0.001639528913074173\n",
      "0.0015312224920489826\n",
      "0.0015197662604623475\n",
      "0.0015322780871065333\n",
      "0.0015351699653547257\n",
      "0.0014969951304374264\n",
      "0.0015775350434705615\n",
      "0.0015409859287319706\n",
      "0.0014540770993335173\n",
      "0.0012801009137183427\n",
      "0.0013969738647574559\n",
      "0.0013127955968957395\n",
      "0.0015149757292238063\n",
      "0.0016156382227200084\n",
      "0.001354041336744558\n",
      "0.0013692330324556678\n",
      "0.0013915364659624174\n",
      "0.001369699532224331\n",
      "0.0012795495183672756\n",
      "0.00142700370575767\n",
      "0.001424939045100473\n",
      "0.001647078132373281\n"
     ]
    }
   ],
   "source": [
    "lossavg = 0\n",
    "\n",
    "\"\"\"\n",
    "One epoch of training through the training data. And periodically prints the average loss \n",
    "(on the training data). It is quite slow, as there are many weights in the network,\n",
    "due to our high input dimension. \n",
    "\"\"\"\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs = torch.flatten(data[0])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    \n",
    "    loss = lossfunc(outputs, inputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lossavg+= loss.item()\n",
    "    if i % 100 == 0 and i != 0:\n",
    "        print(lossavg/100)\n",
    "        lossavg=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error easily converges to around 1E-3. As does the validation error as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029824086019652894\n",
      "0.0014063676772639155\n",
      "0.001421797553775832\n",
      "0.0015576968158711678\n",
      "0.0015116632322315126\n",
      "0.001532444479817059\n",
      "0.0015041677915723995\n",
      "0.0012812459736596792\n",
      "0.0013474100246094168\n",
      "0.001473102539894171\n",
      "0.0014135721075581387\n",
      "0.0014248162019066513\n",
      "0.001583937234536279\n",
      "0.0015198809604044072\n",
      "0.0014858728897524998\n",
      "0.001505210014001932\n",
      "0.0013834289537044242\n",
      "0.0015335165857686662\n",
      "0.001503364919917658\n",
      "0.0014934874454047532\n",
      "0.0016648168058600277\n",
      "0.0015268234658287838\n",
      "0.0014254972751950846\n",
      "0.0014867703424533828\n",
      "0.0014334265748038887\n",
      "0.0014942449645604938\n",
      "0.0015203773477696814\n",
      "0.0015014386596158148\n",
      "0.0014792031282559036\n",
      "0.0013540465600090101\n",
      "0.0014885884837713094\n",
      "0.0014898042191634885\n",
      "0.0013921119458973408\n",
      "0.0014424055145354942\n",
      "0.0013793741242261604\n",
      "0.0015112112552742474\n",
      "0.0013691743827075698\n",
      "0.0014268266048748047\n",
      "0.001461311118910089\n",
      "0.0017029963858658447\n",
      "0.0014592290244763717\n",
      "0.001429881940712221\n",
      "0.0013525637332350016\n",
      "0.001461145468056202\n",
      "0.0013899812774616294\n",
      "0.0013199240347603336\n",
      "0.0013565892985207028\n",
      "0.0014930709500913508\n",
      "0.0013790149334818125\n",
      "0.0014415296725928783\n",
      "0.0015392453398089855\n",
      "0.001537267311650794\n",
      "0.001456684849690646\n",
      "0.0015166592967580073\n",
      "0.001425971060525626\n",
      "0.0015409113926580175\n",
      "0.0013189915765542536\n",
      "0.0014080894563812763\n",
      "0.00151214737968985\n",
      "0.0013663731643464416\n",
      "0.0014581286176689901\n",
      "0.0017968952213414013\n",
      "0.00151448835269548\n",
      "0.0015309263515518978\n",
      "0.0014223072404274716\n",
      "0.0015401436435058713\n",
      "0.001368981827108655\n",
      "0.0013433096520020626\n",
      "0.0014385344009497202\n",
      "0.0014717077897512354\n",
      "0.0013750038054422475\n",
      "0.001414046125137247\n",
      "0.0014189123513642697\n",
      "0.0015062649006722495\n",
      "0.0013956432483973912\n",
      "0.001389103516121395\n",
      "0.0015069894277257845\n",
      "0.001500140260613989\n",
      "0.001445695997099392\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(validation_loader, 0):\n",
    "    inputs = torch.flatten(data[0])\n",
    "    \n",
    "    outputs = model.forward(inputs)\n",
    "    \n",
    "    loss = lossfunc(outputs, inputs)\n",
    "\n",
    "    lossavg+= loss.item()\n",
    "    if i % 100 == 0  and i != 0:\n",
    "        print(lossavg/100)\n",
    "        lossavg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
